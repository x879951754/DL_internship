### 简历上写这些 ###

---

到时候先bb这些。



#### 基本模型 ####

---

**VGG**

（1）用更小的卷积核（3x3）替代AlexNet中的卷积核（11x11），保证相同的感受野的情况下，计算量更少。

（2）用更小的池化核（2x2，stride=2）替换AlexNet中的池化核（3x3）。



**GoogLeNet（Inception）**

提出一种Inception结构，将多个卷积核和池化组装到一起，在GPU下进行并行化训练。减少参数量的同时，加快了网络训练。

<img src="C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220325185443067.png" alt="image-20220325185443067" style="zoom:67%;" />

**ResNet**

解决网络深度越深，网络效果退化的问题。ResNet中提出了一种残差结构，一条路径经过卷积到下一层，而另一条路径“走捷径”直接绕道下一层，这么做的理论依据为如果网络加深，不经过卷积而是直接将特征映射到后一层，这样特征还和原本一样，若再经过残差提取到的特征，实现1+ 0.1 > 1的效果，可以使网络更深。网络更深的好处当然是训练的模型更加拟合特征，更加准确。

残差结构

<img src="C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220325185951121.png" alt="image-20220325185951121" style="zoom: 67%;" />

由于“捷径“和经过卷积的特征通道数可能不同，一般中残差最后一步添加一个1x1卷积调整通道数。注意，最后的两个特征是elementwise add操作。



理论支撑
$$
H(x) = F(x) + x
$$


​		Inception所使用的结合方式是concatenate，合并成一个更大的向量的方式，而ResNet的结合方式是sum。两个结合方式各有优点。concatenate当需要用不同的维度去组合成新观念的时候更有益。而 sum则更适用于并存的判断。



#### 目标检测 ####

---

**Faster RCNN**

![image-20220323103119002](C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220323103119002.png)

Faster RCNN做的事情起始就是，“穷举”一张图片可能出现物体的位置，生成矩形框（计算位置和大小），计算这些框中出现物体的概率，选择概率高的，调整这些矩形框的位置和大小并去除重叠度高的，最终得到一个个包含物体的矩形框。



我们根据上面这幅图，很好地说明了每个步骤。

**特征提取**

Faster RCNN使用预训练的VGG16作为backbone进行特征提取。实现方法是加载预训练模型，抽取并分离前面的卷积层和后面的全连接层。固定卷积层中的部分权重用作特征提取，全连接层则给ROIHead用于分类和回归。



**RPN**

接下来往上走。RPN（Region Proposal Network）获得包含物体的区域。注意RPN是一个网络，负责从图像特征上作筛选，找出一定数量的ROIs。

（1）使用Anchor“穷举“目标物体所在的区域。由VGG16为backbone提取的特征图相对于输入图像缩小的16倍，因此特征图中1个像素点相当于原图16个像素点（1x1区域包含原图16x16的区域）。

正因为如此，如果此时特征图的一个像素点只对应一个anchor，难免出现不能覆盖整个物体情况。于是需要对anchor进行**缩放和形变**。基本尺寸有3种（128x128, 256x256, 512x512），基本长宽比也有3种（1:1, 1:2, 2:1）。这样一共有9种anchor。

具体实现：在VGG16提取的feature map中的一个点（通常是左上角）对应9个anchor的中心点坐标，其他点对应的anchor中心点坐标通过平移计算得出。

但是实现过程中会出现负数，比如最左上角(0, 0)点的坐标有一个anchor，那么anchor的中心点与它对其之后，anchor往左往上的点会溢出到特征图外面，坐标出现负数。这个交给后的ROI的部分来处理（**挖坑**）。



（2）在每个特征点上计算分类和回归结果

这里的**分类**仅仅是二分类（即仅区分前景或者背景）。具体做法：先3x3卷积进一步提取特征，再1x1卷积使通道变为18（18=9x2，对应9个anchor，2个类别），使用softmax计算置信度（概率）。

**回归**的做法，在3x3卷积提取特征之后，用1x1卷积改变通道数，变为36个通道（36=9x4，9个anchor，4=(c_x, c_y, scaled_w, scaled_h)）。



（3）对分类和回归结果进行后处理，生成ROI（Resion of Interest）

到此其实已经完成了检测工作，只不过没有对物体进行细分（只分别了前景和背景），anchor是人工设置的。总的来说就是还不够精细。因此对这一部分进行后处理工作（NMS等），得到ROI。再丢给下个阶段去进行精修。

具体做法是，先将回归得到的Resion Proposal的宽高限制在输入图像尺寸范围内（RPN的第（2）步**填坑**），以及剔除尺寸过小的区域（小于16x16）。再按照前景置信度排序，进行NMS，剔除重复的Resion Proposal。

此时的Resion Proposal就是ROI了。



**ROIHead**

RPN输出的ROI，分类只有前景和背景，bbox的偏移和尺寸前提都是人工设置，因此这一步就需要对ROI进行精修。

（1）对ROI进行池化操作，具体做法是先将ROI缩放到特征图上对应的区域大小（ROI只是一个框内的部分区域），再将ROI划分为7x7个格子，对每个格子内的像素进行（最大/平均）池化。这样ROI Pooling将各个ROI都映射至相同大小。

当然这个步骤会有量化损失：第一次是ROI映射成特征图尺寸范围会有第一次损失（后处理工作（NMS等），得到ROI）。第二次是划分7x7区域的时候，不能整除，又会有第二次损失。



（2）ROI的分类与回归

将ROI Pooling之后的结果展开（flatten）成一维向量，输入全连接层进行分类和回归，对应输出的神经元个数分别为物体类别数（n_classes）和每个类别对应的bbox（n_classes x 4）。

注意：这里是针对预测框中心点相对于正样本ROI中心坐标的偏移，以及两者的长宽比。



**后处理生成预测结果**

后处理具体做法：将网络输出缩放至原图尺寸（注意原图和输入图像之间有区别，输入图像是原图作resize了的），接着对回归的结果去归一化（原本是比例，这里变为绝对值，x 标准差 + 均值），结合ROI的位置和大小计算出bbox的位置（左上角和右下角），并且裁剪到原图尺寸范围内。

然后，选择置信度大于阈值的bbox，使用NMS剔除重叠度高的bbox。

注意：置信度筛选和NMS分别对每个物体单独类别实施的，不包括背景类别。那么就会发生：一个ROI对应不同类别的预测结果都被保留下来（ROIHead输出的是每个ROI在不同类别上的分类和回归）。



**训练**

训练的部分主要包含三个：Backbone、RPN 以及 RoIHead。Backbone 通常会采用在ImageNet上预训练的权重然后进行微调，因此这里主要解析RPN和RoIHead的训练过程，最初的实现将这两部分开训练，后来的大多数实现已使用联合训练的方式。



**RPN训练：**由于anchor数量太多，因此需要筛选部分anchor样本用于指导RPN训练。anchor总样本是Backbone输出特征图上每点对应的9个anchor，从中进行筛选目标样本，具体做法是：

1）将坐标值不在输入图像尺寸范围内的anchor的标签记为-1；

2）将与ground truth（gt）的IoU小于0.3的anchor作为负样本，标签记为0；

3）将与每个gt的IoU最大的anchor作为正样本，标签记为1；

4）将与gt的IoU不小于0.7的anchor作为正样本，标签记为1；

5）限制正样本与负样本总数为256个，正负样本比为1:1，若其中某一类样本超过128个，则随机从中选择多出的样本将其标签记为-1；

6）**仅将标签为0和1的样本用于训练**，忽略标签为-1的anchor

正样本和负样本用作计算分类损失，而回归的损失**仅对正样本**计算。



**ROIHead训练：**这部分是从Proposal Creator （RPN中的Proposal Layer）产生的RoIs中筛选出128个目标样本，其中正负样本比为1:3，用于指导检测器（RoIHead）的训练。

具体方法是，计算每个RoI与每个gt的IoU，若某个RoI与所有gt计算所得的最大IoU不小于0.5，则为正样本，并记录下与此对应的gt，打上相应的类别标签，同时限制正样本数量不超过32个。

相对地，若某个RoI与所有gt的最大IoU小于0.5，则标记为负样本，类别标签为0，同时限制负样本数量不超过96个，正负样本的类别标签用作指导分类训练。最后，计算gt相对于RoI样本的中心点坐标位移和两者长宽比，并且归一化（减均值除标准差），用于指导回归训练。



**损失函数**

这里使用了两种loss函数，CrossEntropy Loss用于分类，Smooth L1 Loss用于回归。注意在RPN和ROIHead中，回归损失均只针对正样本计算。



这里，Smooth L1 Loss 的实现有个技巧，通过给正负样本设置不同的损失权重，其中负样本权重为0，正样本权重为1，这样就可以忽略负样本的回归损失。



损失函数如下（RPN和ROIHead都是这个公式，只不过RPN是2分类）
$$
L(p_i, t_i) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p^*_i) + \lambda \frac{1}{N_{reg}} \sum_i p^*_i L_{reg} (t_i, t^*_i)
$$
其中$N_{cls}$是mini-batch中采集的样本个数（RPN中默认为256个），$N_{reg}$表示anchor位置的数量，即feature map中特征点的数量（约2400个），$\lambda$是平衡参数，相当于加权（大于1时给回归loss加权，小于1时给分类loss加权），论文中默认为10。



$p_i$是预测分类的概率（不论正负样本），正样本标签为1，负样本标签为0。

$L_{cls}(p_i, p^*_i) = -[p^*_i log p_i + (1 - p^*_i) log (1 - p_i)]$



$t_i$和$t^*_i$分别是针对anchor和ground truth所做的参数偏移。

$L_{reg}(t_i, t^*_i) = R(t_i, t^*_i)$，R是Smooth L1 Loss，$p_i L_{reg} (t_i, t^*_i)$表示只有正样本时才回归bbox。



• RPN分类损失：区分anchor是前景还是背景，从而让模型能够学会区分前景和背景；

• RPN回归损失：调整anchor的位置和形状，使其更接近于gt；

• RoI分类损失：区分RoI属于哪个物体类别（这里是21类，包括背景）；

• RoI回归损失：调整RoI的位置和形状，使其更接近于gt、预测结果更精细。

由上述可知，其实在RPN输出的时候，就已经完成了“检测”任务，即能够把目标物体框出来，只不过没有对这些物体类别进行细分而已，并且框出来的位置可能不够精准。而RoIHead可看作是对RPN结果的调优。



![](https://mmbiz.qpic.cn/mmbiz_png/Nabxc8rdYrhsj0dURfv9kmRcg0ibGIBoJaOicriblhRrku1pGnbiaHAbO5QOCX7iaT6xYLM3wfTk4W4b2noIbanhpaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



Faster RCNN总结：

Faster RCNN是基于候选区域的双阶段检测器代表作，总的来说可以分为4个部分：

（1）首先是主干卷积网络的特征提取。VGG16，下采样16倍。

（2）然后是RPN层。①RPN通过softmax判断anchors属于positive或者negative，②利用边框回归修正anchors，③后处理，将region proposal的框的范围限制在输入图像范围内，剔除较小的框（小于16x16），最后再用NMS进行后处理工作。此时的就是输入下一个阶段的ROI了。

（3）ROI Pooling。收集输入的feature map和region proposal。①ROI大小不同，7x7池化统一大小，

（4）再次进行分类（这里是多类别）和回归获得检测框最终精确位置。②flatten成一维输入到全连接层进行分类和回归，③后处理，特征图还原到输入图像大小（输入图像大小和原图不同），去归一化（乘标准差加均值），对前景进行NMS。



**YOLOv3**

总结：

（1）backbone。Darknet53，引入ResNet思想（注意并不是ResNet模块，因为ResNet模块太大）。这里是将ResNet模块的最后一层1x1x256去掉，并将导数第二层3x3x64改成3x3x128。

![img](https://pic3.zhimg.com/80/v2-893db7e58e93152571bc4e53aca0d496_720w.jpg)

（2）多尺度训练。采用金字塔特征图思想，总共输出三种大小的特征图（小尺寸特征图检测大物体，大尺寸特征图检测小物体），第一第二第三层分别下采样32，16，8倍。这三种特征图在经过Darknet53之后（没有全连接层），加入FPN，第一层直接输出，第二层是下采样32倍与下采样16倍的特征图，用卷积层对小的特征图进行上采样之后与大特征图进行concat，第三层同理，将上一层融合后的特征图上采样与当前8倍下采样特征图进行拼接。



（3）Encoder。

输出三种不同大小的特征图，每个都配有3个不同先验框，一共是9种不同大小的先验框。输出通道数是按照bbox（4维），置信度（1维），类别（num_classes维，20或者80维）。

**先验框**

k-means对w，h进行聚类，得到初始的框的大小。



（4）Decoder。对85维解码。

第一：先是对预测框（4维）进行解码。有了先验框与输出的特征图，就可以解码预测框的x, y, w, h。
$$
b_x = \sigma(t_x) + c_x \\
b_y = \sigma(t_y) + c_y \\
b_w = p_w e^{t_w} \\
b_h = p_h e^{t_h}
$$
<img src="https://pic2.zhimg.com/80/v2-758b1df9132a9f4b4e0c7def735e9a11_720w.jpg" alt="img" style="zoom:50%;" />

**网络学习的是$(t_x, t_y, t_w, t_h)$这四个offset。**训练时用$(g_x, g_y, g_w, g_h)$替代$(b_x, b_y, b_w, b_h)$计算出t，然后对t作线性回归。
$$
t_x = g_x - c_x \\
t_y = g_y - c_y \\
t_w = log(\frac{g_w}{p_w}) \\
t_h = log(\frac{g_h}{p_h})
$$
第二：对置信度（1维）进行解码。由sigmoid函数解码，解码之后范围在[0, 1]中。

第三：对类别（80维）进行解码。YOLOv3中用sigmoid替代v2中的softmax，取消了类别之间的互斥。



（5）训练与损失

**划分正负样本和忽略样本（按照置信度）**

正样本：所有m个gt与n个预测框进行配对。每取一个gt，就找预测框中与它iou最大的那个，标记为正样本。之后剩下的m-1个gt再与剩下的n-1个预测框进行配对，直到标出m个预测框。

负样本：与所有gt的iou小于0.5的预测框，归为负样本。

忽略样本：除去正样本后，任何一个与gt的iou大于0.5的，归为忽略样本。

**忽略样本的价值：**（判断正负样本无非是判断置信度，即有无物体）如果给全部的忽略样例置信度标签打0，那么最终的loss函数会变成$Loss_{obj}$和$Loss_{noobj}$的拉扯，不管两个loss数值的权重怎么调整，或者网络预测趋向于大多数预测为负例，或者趋向于大多数预测为正例。而加入了忽略样例之后，网络才可以学习区分正负例。



**损失函数**

三个部分：预测框回归损失，置信度损失，分类损失。

正样本产生3中损失；

负样本标签为0，只有置信度损失；

忽略样本不加入计算。

<img src="https://www.zhihu.com/equation?tex=loss_%7BN_1%7D+%3D+%5Clambda_%7Bbox%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D%5B%28t_x+-+t_x%27%29%5E2+%2B+%28t_y+-+t_y%27%29%5E2%5D%7D+%5C%5C+%2B%5Clambda_%7Bbox%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D%5B%28t_w+-+t_w%27%29%5E2+%2B+%28t_h+-+t_h%27%29%5E2%5D%7D++%5C%5C+-+%5Clambda_%7Bobj%7D%5Csum_%7Bi%3D0%7D%5E%7BN%5Ctimes+N%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7Dlog%28c_%7Bij%7D%29%7D+%5C%5C+-%5Clambda_%7Bnoobj%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bnoobj%7Dlog%281-c_%7Bij%7D%29%7D+%5C%5C+-%7B%5Clambda%7D_%7Bclass%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5C%5C%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D+%5Csum_%7Bc+%5Cin+classes+%7D%5Bp_%7Bij%7D%27%28c%29log%28p_%7Bij%7D%28c%29%29%2B%281-p_%7Bij%7D%27%28c%29%29log%281-p_%7Bij%7D%28c%29%29%5D+%7D+" alt="[公式]" style="zoom:67%;" />

其中回归损失函数是MSE；

置信度损失：正样本就是$\lambda_{obj} \sum^{N*N}_{i=0} \sum^{3}_{j=0} 1^{obj}_{ij} * log(c_{ij})$，负样本$\lambda_{noobj} \sum^{N*N}_{i=0} \sum^{3}_{j=0} 1^{noobj}_{ij} log(1 - c_{ij})$

这个很好理解，N*N大小的特征图，3个通道，不管正负样本，log前面系数都是1，log中$c_{ij}$表示置信度，正负样本都是有多大把握判断该处有物体，显然负样本就有$1 - c_{ij}$的把握此处没有物体。

分类损失：多重BCE。







#### Transformer ####

---

**ViT**

<img src="https://pic1.zhimg.com/v2-0ae5a1ed834f8007016c4492dba7e936_1440w.jpg?source=172ae18b" style="zoom: 50%;" />

ViT-B/16，输入图像大小是224x224x3。

（1）split to patches。灵感来源于NLP Transformer，而NLP Transformer输入是一个序列，所以这里也需要flatten成一维。但是不能flatten成224x224x3，因为计算复杂度与序列长度有关。所以按照p=16切成多个patches，一共有14x14个patches，一个patches是16x16x3的大小。

（2）Linear Projection of Flattened patches。转换成196个768的Patch Embedding。在首位上添加class token（通道拼接），变为(197, 768)，然后给197维token都加上（elementwise add）一个Position Embedding。

（3）transformer Encoder。分为两个部分一个是MSA，另一个是MLP。MSA（Multi-Head Self Attention）和下面Attention的描述一样。MLP由一个FFN（Feed Forward Network）组成，这个FFN包含两个fc层，第一个fc将维度从D变为4D，中间激活函数为GeLU，第二个fc将维度从4D变为D。

图中的+号，类似ResNet一样的skip connection。ResNet中就是相加（H(x) = f(x) + x），这里代码中也是作sum。

（4）MLP Head。根据数据集的不同，采用的全连接层个数也不同。







**Attention**

scaled dot-product attention
$$
Attention(Q, K, V) = softmax(\frac{Q K^T}{\sqrt{d_k}}) V
$$
一维query向量（$q \in R^{d}$），key向量（$k \in R^d$），value向量（$v \in R^d$）

对于一系列的N个query，通过内积计算k个key。因此Q，K，V三个矩阵的shape如下：

Q矩阵（$Q \in R^{N*d}$），K矩阵（$K \in R^{k*d}$），V矩阵（$V \in R^{k * d}$）。注意下图中的Q矩阵shape和另外两个可以不同。当Q，K，V都是从一个包含N个向量的sequence通过线性变换得到的，即$Q = X*W^Q, K = X*W^K, V = X*W^V$，此时就是self-attention，那么N==k，即Q，K，V三个矩阵shape相同。



self-attention计算如下

<img src="C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220324160527265.png" alt="image-20220324160527265" style="zoom: 50%;" />

这里的$\sqrt{d_k}$是key或者value的维度（长度），它作为缩放因子以避免内积带来的方差影响。

<img src="C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220324160553188.png" alt="image-20220324160553188" style="zoom:50%;" />



MSA（Multi-Head Self Attention）

<img src="C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220324161238182.png" alt="image-20220324161238182" style="zoom:50%;" />

MSA代码实现

```python
```







**Swin Transformer**







#### 参考 ####

---

Faster RCNN https://mp.weixin.qq.com/s/ZdGmn4NDbFF9_CzSo3MScQ

YOLOv3 https://zhuanlan.zhihu.com/p/76802514

ViT和Attention https://zhuanlan.zhihu.com/p/356155277

Swin Transformer https://zhuanlan.zhihu.com/p/367111046
