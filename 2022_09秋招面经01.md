## 2022_09秋招面经-01

主要是深度学习在CV方向经常考到的几个点。



### 神经网络

---



**AlexNet**

AlexNet网络一共8层，其中前5层是Conv，后3层是全连接层Fully Connected layer。

（1）ReLU。

（2）Dropout。

（3）Data Augmentation。



**VGG**

VGG利用多个连续的小卷积核替代单个大卷积核，成功地说明了网络结构还是深的效果好。

（1）采用连续几个3x3大小的卷积核代替AlexNet中的大卷积核（11x11, 5x5）。

（2）池化核减小。AlexNet中的池化核大小为3x3，stride=2。而VGG中池化核大小为2x2，stride为2。



**GoogLeNet/Inception**

Inception模块结构。将多个卷积或池化操作串联或并行整合成一个模块，加快计算。



**ResNet**

![img](https://pic1.zhimg.com/80/v2-0892e5423616c30f69ded61111b111c0_720w.jpg)



**MobileNet**

深度可分离卷积 = 深度卷积 + 逐点卷积



**ShuffleNet**

<https://zhuanlan.zhihu.com/p/32304419>

![](https://pic3.zhimg.com/80/v2-f8ddc33fb1f578bf7f51a4b6e5407426_720w.jpg)

ShuffleNet的核心设计理念是对不同的channels进行shuffle来解决group convolution带来的弊端。



ShuffleBlock的设计

![](https://pic1.zhimg.com/80/v2-a8d5297a16f7bcc40a31d427cf062e58_720w.jpg)

就是在GConv和DWConv之间加了一个Channel Shuffle块。



### 常用损失函数

---



**0-1损失函数**
$$
L(y, f(x)) = \left \{ \begin{array}{c} 
	0, & y == f(x) \\
	1, & y != f(x)
\end{array} \right.
$$
或者
$$
L(y, f(x)) = \left \{ \begin{array}{c} 
	0, & |y - f(x)| < \sigma \\
	1, & |y - f(x)| >= \sigma
\end{array} \right.
$$
**绝对值损失函数（L1 loss）**
$$
L(y, f(x)) = |y - f(x)|
$$
或者
$$
L = \frac{1}{n} \sum^n_{i=1} |y_i - \hat{y_i}|
$$


**平方损失函数（L2 loss）**
$$
L(y, f(x)) = (y - f(x))^2
$$
或者
$$
L = \frac{1}{n} \sum^n_{i=1} (y_i - \hat{y_i})^2
$$
**smooth L1 loss**
$$
smooth \ \ L_1 = \left\{ \begin{array}{c} 
0.5x^2, & |x| < 1 \\
|x| - 0.5, & otherwise
\end{array} \right. \\

smooth \ \ L_1^{'} = \left\{ \begin{array}{c} 
-1, & x < -1 \\
-x, & -1 <= x < 0 \\
x, & 0 <= x < 1 \\
1, & 1 <= x
\end{array} \right.
$$




### 常用的激活函数

---





### 几个容易忽略的知识点

---



**dropout训练和测试的区别**

理论上，训练时以概率p屏蔽一些神经元，测试时采用全部神经元进行推理，理应将每个神经元的权重乘上概率p。

实际上，训练时保存的神经元权重除以(1 - p)，测试时不用做任何操作。



**权重初始化**



**Q: anchor-free存在什么缺点？**

**A: **这里只能想到两种：1.正负样本极端不平衡；2.语义模糊性（两个目标中心点重叠）；现在这两者大多是采用Focus Loss和FPN来缓解的，但并没有真正解决。

https://mp.weixin.qq.com/s/hPDcxAq28d_cSXOXBcnQIw











### 面试中的手撕代码

---

* 搭建神经网络


```python
import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self, input_channel, output_channel):
        super(Net, self).__init__()
        
        self.conv = nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=0, bias=False),
        self.relu = nn.ReLU(inplace=True),
        self.bn = nn.BatchNorm2d(output_channel)
        self.pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=0)
        self.fc = nn.Linear(output_channel)
        
    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        x = self.bn(x)
        x = self.pool(x)
        out = self.fc(x)
        return out
        
```




* 中值滤波


```python

```




* sobel滤波器


```python
def sobel_filter(gray, ksize=3):
    H, W = gray.shape[:2]
    pad = ksize // 2
    out = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float32)
    out[pad:pad+H, pad:pad+W] = gray.copy()
    tmp = out.copy()
    
    out_v = out.copy()
    out_h = out.copy()
    
    kv = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]
    kh = [[1, 0, -1], [2, 0, -2], [1, 0, -1]]
    
    for y in range(H):
        for x in range(W):
            out_v[pad + y, pad + x] = np.sum(kv * tmp[y: y+ ksize, x: x+ksize])
            out_h[pad + y, pad+ x] = np.sum(kh * tmp[y: y + ksize, x:x + ksize])
    
    out_v = np.clip(out_v, 0, 255)
    out_h = np.clip(out_h, 0, 255)
    
    out_v = out_v[pad:pad+H, pad:pad+W].astype(np.uint8)
    out_h = out_h[pad:pad+H, pad:pad+W].astype(np.uint8)
    
    return out_v, out_h
```



* 计算iou

```python
def box_iou(boxes1, boxes2):
  '''
  :param boxes1: torch.tensor(), [[x1, y1, x2, y2], [], ...]
  :param boxes2: torch.tensor(), [[x1, y1, x2, y2], [], ...]
  :return:
  '''
  area1 = box_area(boxes1)
  area2 = box_area(boxes2)
  
  # [N, M, 2]
  lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])
  rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
  
  wh = (rb - lt).clamp(min=0)  # [N, M, 2]
  inter = wh[:, :, 0] * wh[:, :, 1]  # [N, M]
  union = area1[:, None] + area2 - inter
  iou = inter / union
  return iou

def box_area(boxes):
  return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
```




* nms代码


```python
def nms(bboxes, scores, nms_thresh=0.5):
  '''
  :param bboxes:
  :param scores:
  :param nms_thresh:
  :return:
  '''
  x1 = bboxes[:, 0]
  y1 = bboxes[:, 1]
  x2 = bboxes[:, 2]
  y2 = bboxes[:, 3]
  
  areas = (x2 - x1 + 1) * (y2 - y1 + 1)
  order = scores.argsort()[::-1]
  
  keep = []
  while order.size > 0:
    i = order[0]
    keep.append(i)
    
    xx1 = np.maximum(x1[i], x1[order[1:]])
    yy1 = np.maximum(y1[i], y1[order[1:]])
    xx2 = np.minimum(x2[i], x2[order[1:]])
    yy2 = np.minimum(y2[i], y2[order[1:]])
    
    w = np.maximum(1e-8, xx2 - xx1)
    h = np.maximum(1e-8, yy2 - yy1)
    inter = w * h
    
    ovr = inter / (areas[i] + areas[order[1:]] - inter)
    inds = np.where(ovr < nms_thresh)[0]
    order = order[inds + 1]
  return keep

if __name__ == '__main__':
  bboxes = np.array([[10, 10, 110, 110], [150, 150, 250, 250], [120, 120, 220, 220], [120, 150, 220, 250]])
  scores = np.array([0.75, 0.56, 0.81, 0.46])
  output = nms(bboxes, scores)
  print(output)
```



* soft-nms

```python
import numpy as np
from copy import deepcopy

def soft_nms(bboxes, nt=0.3, sigma2=0.5, score_thresh=0.5, method=2):
  '''
  :param bboxes: nd.array()
  :param nt: iou_thresh 
  :param sigma2: gaussian_sigma^2
  :param score_thresh: confidence
  :param method: method index
  :return:
  '''
  res_bboxes = deepcopy(bboxes)
  # bbox总数
  n = bboxes.shape[0]
  # 下标[0, 1, ..., n - 1]
  indices = np.array([np.arange(n)])
  bboxes = np.concatenate((bboxes, indices.T), axis=1)
  
  x1 = bboxes[:, 0]
  y1 = bboxes[:, 1]
  x2 = bboxes[:, 2]
  y2 = bboxes[:, 3]
  scores = bboxes[:, 4]
  areas = (x2 - x1 + 1) * (y2 - y1 + 1)
  
  for i in range(n):
    # 找出 i 之后的最大 score 及其下标
    pos = i + 1
    if i != n - 1:
      maxscore = np.max(scores[pos:], axis=0)
      maxpos = np.argmax(scores[pos:], axis=0)
    else:
      maxscore = scores[-1]
      maxpos = 0
    
    # 如果当前 i 的得分小于后面的最大 score, 则与之交换, 确保 i 上的 score 最大
    if scores[i] < maxscore:
      bboxes[[i, maxpos + i + 1]] = bboxes[[maxpos + i + 1, i]]
      scores[[i, maxpos + i + 1]] = scores[[maxpos + i + 1, i]]
      areas[[i, maxpos + i + 1]] = areas[[maxpos + i + 1, i]]
    
    # iou calculate
    xx1 = np.maximum(bboxes[i, 0], bboxes[pos:, 0])
    yy1 = np.maximum(bboxes[i, 1], bboxes[pos:, 1])
    xx2 = np.minimum(bboxes[i, 2], bboxes[pos:, 2])
    yy2 = np.minimum(bboxes[i, 3], bboxes[pos:, 3])
    
    w = np.maximum(0.0, xx2 - xx1 + 1)
    h = np.maximum(0.0, yy2 - yy1 + 1)
    intersection = w * h
    iou = intersection / (areas[i] + areas[pos:] - intersection)
    
    # three method: 1.linear 2.gaussian 3.original nms
    if method == 1:
      weight = np.ones(iou.shape)
      weight[iou > nt] = weight[iou > nt] - iou[iou > nt]
    elif method == 2:
      weight = np.exp(-(iou * iou) / sigma2)
    else:
      weight = np.ones(iou.shape)
      weight[iou > nt] = 0
    scores[pos:] = weight *scores[pos:]
  
  # select the boxes and keep the corresponding indices
  inds = bboxes[:, 5][scores > score_thresh]
  keep = inds.astype(int)
  return res_bboxes[keep]

if __name__ == '__main__':
  bboxes = np.array([[10, 10, 110, 110, 0.75], [150, 150, 250, 250, 0.56], [120, 120, 220, 220, 0.81], [120, 150, 220, 250, 0.46]])
  output = soft_nms(bboxes=bboxes)
  print(output)
```



### GAN

---



loss

![](https://img-blog.csdnimg.cn/img_convert/af69f6e9eee1c904245e313edc4c5931.png)

<https://blog.csdn.net/weixin_36309562/article/details/112292761>



KL散度（Kullback-Leibler Divergence）一般用于度量两个概率分布函数之间的“距离”。
$$
KL(P(X), Q(X)) = \sum_{x \in X} [P(X) log \frac{P(x)}{Q(x)}] = E_{x ~ P(x)} [log \frac{P(x)}{Q(x)}]
$$
但KL散度是不对称的。不对称意味着，对于同一个距离，观察方式不同，获取的loss也不同，那么整体loss下降的方向就会趋向于某个特定方向。这在GAN中非常容易造成模式崩塌，即生成数据的多样性不足。

JS散度在KL散度的基础上进行了修正，保证了距离的对称性：



实际上，无论KL散度还是JS散度，在直接用作loss时，都是难以训练的：由于分布只能通过取样计算，这个loss在每次迭代时都几乎为零。

GAN的训练方法，能够巧妙的解决这个问题：先训练D，再训练G，二者相互对抗，直到收敛。

**原始GAN的loss实际等价于JS散度**





### SRGAN

---



深度特征度量图像相似度的有效性——LPIPS



FID是从原始图像的计算机视觉特征的统计方面，来衡量两组图像的相似度，是计算真实图像和真实图像的特征向量之间距离的一种度量。这种视觉特征是使用Inception v3图像分类模型提取特征并计算得到的。分数越低两组图像越相似。



去噪 TV Loss

![](https://img-blog.csdnimg.cn/20190716120750754.png)

即：求每一个像素和横向下一个像素的差的平方，加上纵向下一个像素的差的平方。然后开β/2次根。

<https://blog.csdn.net/qq_33590958/article/details/96119966>



ESRGAN

1.resblock换成denseblock，按照残差跳跃方式连接

2.去掉BN

3.relativistic Discriminator https://blog.csdn.net/weipf8/article/details/106436583

4.感知损失Lpercep与SRGAN不同的是，使用激活前的feature map作为重构特征。



<https://zhuanlan.zhihu.com/p/151809787>



GFPGAN



![](https://img-blog.csdnimg.cn/9e1417eea7c54caf81089846015a2e6c.png)





![](https://img-blog.csdnimg.cn/b68d9b0c3b174ba9a56be2e86e48053c.png)



<https://blog.csdn.net/weixin_42975688/article/details/126065310>



**PSNR**
$$
MSE = \frac{1}{mn} \sum^{m-1}_{i=0} \sum^{n-1}_{j=0} ||I(i,j) - K(i,j)||^2 \\
PSNR = 10 * log_{10}[\frac{(2^n - 1)^2}{MSE}] = 20 * log_{10}[\frac{2^n - 1}{\sqrt{MSE}}]
$$
n是图像位数，8位图像像素值上限为$$2^8 - 1 = 255$$

**SSIM**

我们通过将两个图像分成小块来计算 SSIM，然后采用**滑动窗口**的思想**逐块**比较图像。给定来自一个图像的patch x和来自另一张图像的相应patch y，我们计算每个patch中像素强度的以下汇总统计信息。

![](https://pic2.zhimg.com/80/v2-f3e5c3fda51b19705b814e9d9abcb6f1_720w.webp)

计算两个patch之间的亮度（luminance）相似性。
$$
l(x, y) = \frac{2 \mu_x \mu_y + C_1}{\mu^2_x + \mu^2_y + C_1}
$$
计算对比（contrast）相似度得分。
$$
c(x, y) = \frac{2 \sigma_x \sigma_y + C_2}{\sigma^2_x + \sigma^2_y + C_2}
$$
计算结构（structure）分数，这是两个patch中像素值之间的相关性。
$$
s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3}
$$
总的 SSIM 分数是这三个分数的乘积，当参数$$ C_3 = \frac{C_2}{2} $$时，最终的表达式如下：
$$
SSIM(x, y) = l(x, y) c(x, y) s(x, y) = \frac{2 \mu_x \mu_y + C_1}{\mu^2_x + \mu^2_y + C_1} * \frac{2 \sigma_{xy} + C_2}{\sigma^2_x + \sigma^2_y + C_2}
$$





### mAP计算方法

---

在VOC2010以前，只需要选取当Recall >= 0, 0.1, 0.2, …, 1共11个点时的Precision最大值，然后AP就是这11个Precision的平均值。

在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值。

<https://blog.csdn.net/qq_35705332/article/details/109028620>





### 方差，协方差

---

期望：$E(X) = \mu, E(Y) = \xi$

方差（自己与本身作协方差）：
$$
Var(X) = E((X - \mu)^2) \\
= E(X^2 - 2 \mu X + \mu^2) \\
= E(X^2) - 2\mu E(X) + E(\mu^2) \\
= E(X^2) - 2 E(X) E(X) + \mu^2 \\
= E(X^2) - 2 E^2(X) + E^2(X) \\
= E(X^2) - E^2(X)
$$
协方差：
$$
Cov(X, Y) = E((X - \mu) * (Y - \xi)) \\
= E(XY - \xi X - \mu Y + \mu \xi) \\
= E(XY) - E(\xi X) - E(\mu Y) + E(\mu \xi) \\
= E(XY) - \mu \xi - \mu \xi + \mu \xi \\
= E(XY) - \mu \xi
$$
如果X与Y是独立统计的，那么$E(XY) = E(X)*E(Y) = \mu \xi$，则协方差为0；但是反过来，二者协方差为0，不一定独立统计。