### 机器学习 ###

---

常用的机器学习算法，包括概念，推导和经常问的问题。



#### 逻辑回归 ####

---

**Q：推导一下逻辑回归的损失函数，并解释其含义**

设
$$
P(Y=1|x) = p(x) \\
P(Y = 0|x) = 1 - p(x)
$$
似然函数：
$$
L(w) = \prod [p(x_i)]^{y_i}[1 - p(x_i)]^{(1 - y_i)}
$$
负对数似然函数：
$$
-log(L(w)) = -log(\prod [p(x_i)]^{y_i}[1 - p(x_i)]^{(1 - y_i)}) \\
= - \sum log([p(x_i)]^{y_i}[1 - p(x_i)]^{(1 - y_i)}) \\
= - \sum y_i log p(x_i) + (1 - y_i) log (1 - p(x_i))
$$
这样就得到了损失函数：
$$
L(w) = - \sum y_i log p(x_i) + (1 - y_i) log (1 - p(x_i))
$$
最大化似然函数就是最小化交叉熵误差，现在求解这个损失函数的最小值。先不考虑累加和，针对每一个参数$w_i$求偏导。（这里的推断可以看下面**交叉熵的求导**一节中的推理）
$$
\frac{\partial L(w)}{\partial w_i} = (y_i \frac{1}{p(x_i)} - (1 - y_i) \frac{1}{1 - p(x_i)}) \frac{\partial p(x_i)}{\partial w_i} \\
= (\frac{y_i(1 - p(x_i)) - (1 -  y_i)p(x_i)}{p(x_i)(1 - p(x_i))}) p(x_i)(1 - p(x_i)) \frac{\partial (w^T x)}{\partial w_i} \\
= (y_i - p(x_i)) \frac{\partial (w^T x)}{\partial w_i} \\
w^T x中只有一项对应的w_i，即w_i^T x_i这一项，求导之后为x_i \\
= (y_i - p(x_i)) x_i
$$
使用梯度下降法，对w每次更新
$$
w_j := w_j + \eta \frac{\partial L(w)}{\partial w_i} \\
w_j := w_j + \eta (y_i - p(x_i)) x_i
$$


**Q：为什么 LR 模型要使用 sigmoid 函数，背后的数学原理是什么？为什么不用其他函数？**

使用LR模型的前提是：数据服从伯努利分布（两点分布，二项分布是n重伯努利实验成功次数的离散分布）。

只有2种情况，$p(y=1)=\pi_1$和$p(y=0)=\pi_0$，伯努利实验成功的概率为$p(y=1)$的概率，即
$$
p(y=1|x) = \frac{\pi_1 N(x|u_1, \sum_1)}{\pi_1 N(x|u_1, \sum_1) + \pi_0 N(x|u_0, \sum_0)} \\
= \frac{1}{1 + \frac{\pi_0 N(x|u_0, \sum_0)}{\pi_1 N(x|u_1, \sum_1)}} \\
= \frac{1}{1 + \frac{1 - p(y=1|x)}{p(y=1|x)}}
$$
而$\frac{1 - p(y=1|x)}{p(y=1|x)}$的倒数$\frac{p(y=1|x)}{1 - p(y=1|x)}$称作比率，且$p(y=1|x) \in [0, 1]$，它的曲线是近似$y=e^x$，故可以用$y=e^{w^Tx + b}$近似代替
$$
\frac{p(y=1|x)}{1 - p(y=1|x)} = e^{w^Tx + b} \\
\frac{1 - p(y=1|x)}{p(y=1|x)} = e^{-(w^Tx + b)}
$$
带回到原式子中
$$
p(y=1|x) = \frac{1}{1 + e^{-(w^Tx + b)}}
$$
最后说一句，逻辑回归的模型引入了sigmoid函数映射，是非线性模型，但本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。可以说，逻辑回归，都是以线性回归为理论支持的。



**Q：交叉熵函数求导**

激活函数
$$
L = -[y log(p) + (1 - y) log(1 - p)]
$$
又已知sigmoid函数
$$
p = \sigma(s) = \frac{1}{1 + e^{-s}} = \frac{e^s}{e^s + 1} \\
s = w^T x + b
$$
通过这两个式子，我们可知参数的传递情况：L（损失函数）-> p（概率）-> s（score）-> w（权重）。

我们用链式求导法则：
$$
\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial p} \frac{\partial p}{\partial s} \frac{\partial s}{\partial w_i}
$$
分别对每个部分求导，最后相乘

$\frac{\partial L}{\partial p}$为
$$
\frac{\partial L}{\partial p} = - [y \frac{1}{p} - (1 - y) \frac{1}{1 - p}] \\
= - y \frac{1}{p} + (1 - y) \frac{1}{1 - p}
$$
$\frac{\partial p}{\partial s}$为
$$
\frac{\partial p}{\partial s} = \frac{e^s(e^s + 1) - (e^s)^2}{(e^s + 1)^2} \\
= \frac{e^s}{(e^s + 1)^2} \\
= \frac{e^s + 1}{(e^s + 1)^2} - \frac{1}{(e^s + 1)^2} \\
= \frac{1}{e^s + 1} - \frac{1}{(e^s + 1)^2} \\
= \sigma(s) - \sigma^2(s) \\
= \sigma(s)(1 - \sigma(s))
$$
$\frac{\partial s}{\partial w_i}$为
$$
\frac{\partial s}{\partial w_i} = x_i
$$
三者相乘为
$$
[- y \frac{1}{p} + (1 - y) \frac{1}{1 - p}] * \sigma(s)(1 - \sigma(s)) * x_i \\
= [\frac{-y(1 - p) + p(1 - y)}{p (1 - p)}] * \sigma(s)(1 - \sigma(s)) * x_i
$$
先将$p = \sigma(s)$带入
$$
[\frac{-y(1 - \sigma(s)) + \sigma(s)(1 - y)}{\sigma(s) (1 - \sigma(s))}] * \sigma(s)(1 - \sigma(s)) * x_i \\
= (\sigma(s) - y) * x_i \\
= (p - y) * x_i
$$
参考 https://zhuanlan.zhihu.com/p/99923080



**Q：如果求逻辑回归的参数？**

这个问题很简单，将上面逻辑回归交叉熵的导数求出来之后，求损失函数最小值，令导数为0，求得每次更新点。利用梯度下降法$w_j := w_j + \eta \frac{\partial L}{\partial w}$，每轮对w进行更新。



**Q：SVM 和 LR 有什么异同？分别在什么情况下使用？**

（1）从目标函数（损失函数）来看，逻辑回归采用对数损失函数（log loss），而SVM采用的是hinge loss。

先写出两种损失函数的表达式，其中$f(x) = w^T x + b$，那么有
$$
L_{log} = -[y log(f(x)) + (1 - y) log(1 - f(x))] \\
L_{hinge} = max(0, 1 - y f(x))
$$
从而写出两者的损失函数，对所有的样本点取均值
$$
LR: L = \frac{1}{n} \sum^{n}_{i} -[y_i log(w_i^T x_i + b) + (1 - y_i) log(1 - (w_i^T x_i + b))] \\
SVM: L = \frac{1}{n} \sum^{n}_{i} [1 - y_i (w_i^T + x_i + b)]
$$





#### 贝叶斯 ####

---

**Q：什么是贝叶斯决策论？**

贝叶斯决策论是概率框架下实施决策的基本方法。要了解贝叶斯决策论，首先得了解以下几个概念：先验概率，条件概率、后验概率、误判损失、条件风险、贝叶斯判别准则。

- 先验概率：

所谓先验概率，就是根据以往的经验或者现有数据的分析所得到的概率。如，随机扔一枚硬币，则p(正面) = p(反面) = 1/2，这是我们根据已知的知识所知道的信息，即p(正面) = 1/2为先验概率。



- 条件概率：

所谓条件概率是指事件A在另一事件B发生的条件下发生的概率。用数学符号表示为：P(B|A)，即B在A发生的条件下发生的概率。举个栗子，你早上误喝了一瓶过期了的牛奶（A），那我们来算一下你今天拉肚子的概率（B），这个就叫做条件概率。即P（拉肚子|喝了过期牛奶）， 易见，条件概率是有因求果（知道原因推测结果）。



- 后验概率：

后验概率跟条件概率的表达形式有点相似。数学表达式为p(A|B), 即A在B发生的条件下发生的概率。以误喝牛奶的例子为例，现在知道了你今天拉肚子了（B），算一下你早上误喝了一瓶过期了的牛奶(A)的概率, 即P（A|B），这就是后验概率，后验概率是有果求因（知道结果推出原因）



- 误判损失：

数学表达式：$L(j|i)$

误判损失表示把一个标记为i类的样本误分类为j类所造成的损失。比如，当你去参加体检时，明明你各项指标都是正常的，但是医生却把你分为癌症病人，这就造成了误判损失，用数学表示为：$L(癌症|正常)$



- 条件风险：

是指基于后验概率$P(i|x)$，可获得将样本x分类为i所产生的期望损失，公式为$R(i|x) = \sum{L(i|j)}{P(j|x)}$。

（其实就是所有判别损失的**加权和**，而这个**权**就是样本判为j类的概率，即$P(j|x)$。样本本来应该含有$P(j|x)$的概率判为j类，但是却判为了i类，这就造成了错判损失。而将所有错判损失与正确判断的概率的乘积相加，就能得到样本错判为i类样本的平均损失，即条件风险。）
$$
R(i|x) = L(i|a)*P(a|x) + L(i|b)*P(b|x) + L(i|c)*P(c|x)
$$
其中$a, b, c$分别为判别的类别。



- 贝叶斯判别准则：

贝叶斯判别准则是找到一个使条件风险达到最小的判别方法。即，将样本判为哪一类，所得到的条件风险$R(i|x)$（或者说平均判别损失）最小，那就将样本归为那个造成平均判别损失最小的类。

此时$h^*(x) = argmin R(i|x)$就称为贝叶斯最优分类器。



**总结：**

贝叶斯决策论是基于先验概率求解后验概率的方法，其核心是寻找一个判别准则，使得条件风险达到最小。

而在最小化分类错误率的目标下，贝叶斯最优分类器又可以转化为求后验概率达到最大的类别标记，即
$$
h^*(x) = argmax P(i|x), \\ 此时 \left\{ \begin{array}{c} L(i|j) = 0, & if \ \ i=j \\ L(i|j) = 1, & otherwise \end{array} \right.
$$





**Q： 你能给我说说朴素贝叶斯有什么优缺点吗？**

朴素贝叶斯的优点：

（1）古典数学理论，有稳定的分类效率。分类准确率高，速度快。

（2）对缺失数据不太敏感，算法也比较简单，常用于文本分类。

（3）对小规模数据表现很好，能处理多分类任务，适合增量式训练，当数据量超出内存时，我们可以一批批的去增量训练。（朴素贝叶斯在训练过程中只需要计算各个类的概率和各个属性的类的概率，这些概率值可以快速地根据增量数据进行更新，无需重新全量计算）

朴素贝叶斯的缺点：

（1）对训练数据的依赖性很强，如果模型在训练集上的误差较大，那么预测出来的效果就会不佳。

（2）理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但在实际中，因为朴素贝叶斯“朴素”的特点，导致在属性比较多或者属性间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。

（3）需要知道先验概率，且先验概率很多时候是基于假设或者已有的训练数据所得的，这在某些时候会因为假设先验概率的原因上出现分类决策上的错误。



**Q：朴素贝叶斯缺点明显，为什么朴素贝叶斯的预测仍然可以取得较好的效果？**

（1）对于分类任务来说，只要按照概率大小将每个类别的概率排序正确，那么就可以通过概率大小来判断类别，并不需要知道精确概率。（朴素贝叶斯的核心是找出后验概率最大的那个类别，而不是求出精确的概率值）

（2）如果属性之间的相互依赖对所有类别的影响相同，或者相互依赖关系可以相互抵消，那么属性条件独立性假设在降低计算开销的同时不会对分类结果产生不良影响。



**Q： 什么是拉普拉斯平滑法？**

拉普拉斯是处理朴素贝叶斯中零概率的一种方式。某些属性可能在训练集中没有与某个类同时出现过，为了防止其他属性所携带的信息被这个在训练集中未出现的属性“抹去”，所以采用拉普拉斯平滑法（拉普拉斯估计器）。

具体操作：

1）首先分子都+1；

2）对于先验概率：分母上加上训练集中可能的类别数；对于后验概率，在分母上加上这个属性可能的取值数。



**Q：朴素贝叶斯中有没有超参数可以调？**

朴素贝叶斯是没有超参数可以调的，所以它不需要调参，朴素贝叶斯是根据训练集进行分类，分类出来的结果基本上就是确定了的。

拉普拉斯估计器不是朴素贝叶斯中的参数，不能通过拉普拉斯估计器来对朴素贝叶斯调参。



**Q：朴素贝叶斯中有多少种模型？**

朴素贝叶斯的3中模型：

（1）高斯模型。对连续型数据进行处理。（线性回归）

（2）多项式模型。对离散型数据进行处理，计算数据的条件概率。（使用拉普拉斯估计器进行平滑的一个模型）

（3）伯努利模型。伯努利模型的取值特征是布尔型，即出现为true，不出现为false。在进行文档分类时，就是一个单词有没有在一个文档里出现过。



**Q：你知道朴素贝叶斯有哪些应用吗？**

朴素贝叶斯的应用最广的应该就是在文档分类、垃圾文本过滤（如垃圾邮件、垃圾信息等）、情感分析（微博、论坛上的积极、消极等情绪判别）这些方面，除此之外还有多分类实时预测、推荐系统（贝叶斯与协同过滤组合使用）、拼写矫正（当你输入一个错误单词时，可以通过文档库中出现的概率对你的输入进行矫正）等。



**Q：朴素贝叶斯是高方差还是低方差模型？**

对于简单模型来说，偏差很大，方差会较小；复杂模型相反，偏差较小，方差很大。朴素贝叶斯假设了各个属性之间是相互独立的，算是一个简单的模型。因此朴素贝叶斯的偏差较大，方差较小，是低方差模型。

（偏差是模型预测值与真实值之间的误差，即模型的精准度；方差是模型预测值与模型预测期望之间的误差，即模型的稳定性，数据集中性的一个指标）



**Q：朴素贝叶斯的假设条件是什么？优缺点分别是什么？**

朴素贝叶斯对条件概率分布做了**条件独立性**的假设，即特征之间是相互独立的。

但一般情况下，样本的特征之间独立这个条件的确是弱成立的，尤其是数据量非常大的时候。虽然牺牲了准确性，但是模型条件分布的计算大大简化了（拉普拉斯平滑法，估计器），这是贝叶斯模型的选择。



**Q：朴素贝叶斯是如何进行参数估计的？**

先求$P(Y=C_k)$，通过极大似然估计求得$P(Y=C_k)$为样本类别$C_k$出现的频率，即$\frac{m_k}{m}$。其中$m_k$为样本类别$C_k$的出现次数，总样本数为$m$。

这里说一下，**$X_j$是样本点中第j个特征**。现在求$P(X_j = X_j^{test} | Y = C_k)$，取决于先验条件：

情况（1）：如果$X_j$是**离散值**，前面一个问题中的多项式模型，设$x_j$符合多项式分布，这样得到
$$
P(X_j = X_j^{test} | Y = C_k) = \frac{P(X_j = X_j^{test})P(Y = C_k | X_j = X_j^{test})}{P(Y = C_k)} \\
= \frac{m_{kj}^{test}}{m_k}
$$
其中$m_k$是样本类别为$C_k$的总的特征数，$m_{kj}^{test}$为类别为$C_k$的样本中，第$j$维特征$X_j^{test}$出现的计数。

---

$$
P(A|B) = \frac{P(A) * P(B|A)}{P(B)}
$$

---

**如果某些类别在样本中没有出现**，导致上式结果为0，引入拉普拉斯平滑法（估计器）：
$$
P(X_j = X_j^{test} | Y = C_k) = \frac{m_{kj}^{test} + \lambda}{m_k + O_j \lambda}
$$
其中$\lambda$为一个大于0的常数，常常取为1。$O_j$为第j个特征的取值个数。



情况（2）：如果$X_j$是非常稀疏的离散值，伯努利模型，特征$X_j$出现为1，不出现为0。即只要$X_j$出现，不关注$X_j$的出现次数。
$$
P(X_j = X_j^{test} | Y = C_k) = P(X_j = 1| Y = C_k)X_j^{test} + [1 - P(X_j = 1 | Y = C_k)] (1 - X_j^{test})
$$
其中$X_j^{test}$取值为0和1。



情况（3）：如果$X_j$是连续值，高斯模型，取$X_j$的先验概率服从正态分布。即在样本类别$C_k$中，$X_j$的值符合正态分布。这里提一下高斯概率分布
$$
Gaussian(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{\frac{-(x - \mu)^2}{2 \sigma^2}}
$$
那么
$$
P(X_j = X_j^{test} | Y = C_k) = \frac{1}{\sqrt{2\pi} \sigma_k} e^{\frac{-(X_j^{test} - \mu_k)^2}{2 \sigma_k^2}}
$$
其中$\mu_k$和$\sigma_k^2$是正态分布的期望和方差，可以通过极大似然估计求得。$\mu_k$为在样本类别中，所有$X_j$的平均值。$\sigma_k^2$为在样本类别$C_k$中，所有$X_j$的方差。对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。



**Q：贝叶斯学派与频率学派有何不同？**

频率学派，其特征是把需要推断的参数θ视作固定且未知的常数，而样本X是随机的，其着眼点在样本空间，有关的概率计算都是针对X的分布。

贝叶斯学派，他们把参数θ视作随机变量，而样本X是固定的，其着眼点在参数空间，重视参数θ的分布，固定的操作模式是通过参数的先验分布结合样本信息得到参数的后验分布。



用我自己的理解：

对于频率派来说，一个模型中的参数是“固定”的（**参数空间不变**），而数据样本是在分布中随机采样的。这里的”固定“是指：

模型或者一个分布中的参数是固定不变的，而观察（采样）到的每个数据是这个分布中的一个独立同分布样本。即服从于这个参数分布，不管如何采样，根据参数对其估计不变。如果根据数据估计出来的参数与真实模型不符，那也只是引入了噪声而已。

对于贝叶斯派来说，观察（采样）到的数据样本才是“固定”的（**即样本空间不变**），而模型参数一直在变化。刚开始对模型的参数有一个最初始的假设，即**先验假设**，一旦设置之后，才可以由观察到的数据样本对模型参数进行更新。
$$
P(\theta|D) = \frac{P(D | \theta)P(\theta)}{P(D)}
$$
其中$P(\theta)$是先验概率，指刚开始引入的专家知识（专家知识指导$\theta$的学习）；$P(\theta|D)$是后验概率，由观察的数据样本和先验知识推导出来的参数分布；$P(D|\theta)$是似然函数，指的是由观察数据导致的参数更新。

这种观点下，模型参数不再是一个参数，而是一个分布。

**总结**

频率派：参数固定（参数空间不变），样本在样本空间中随机分布。

贝叶斯派：样本固定（样本空间不变），而参数在参数空间中是随机分布。



**Q：逻辑回归与朴素贝叶斯有什么区别？**

朴素贝叶斯是生成模型，从数据样本中模拟先验概率和后验概率；而逻辑回归是判别式模型，用标签与预测值进行模拟。

朴素贝叶斯的前提是**条件独立性**假设，数据样本的各个属性特征之间独立，因此朴素贝叶斯可以不用梯度下降，而直接通过统计每个特征的逻辑发生比来当做权重；逻辑回归没有这个前提，直接估计参数w，对分类结果进行判断。使用梯度下降法，去拟合每个特征之间的耦合信息，从而更新权重w。归一化的LR对应于最大化后验概率。

**总结**

NB的前提需要数据样本之间的特征符合条件独立性，而LR不需要这个前提，LR会在训练过程中对w参数进行更新。

NB正是由于有条件独立性这个假设的前提，可以进行先验估计，因此后续的计算可以直接通过特征之间的比率进行计算，因此收敛速度更快；而LR没有这个前提，收敛速度慢。

NB对于数量量小的情况，特征之间的条件独立性假设成立，模型效果相较于LR更好；在数据量更多的情况下，特征之间的条件独立性假设难以证明，因此LR模型效果更好。



#### 聚类 ####

---

**Kmeans**

（1）原理：对于给定的样本集，按照样本之间的距离大小，将样本划分为若干个簇，使簇内距离尽可能小，簇间距离尽可能大。



（2）步骤：

1.随机选择k个样本作为初始簇内中心；

2.计算样本到各个聚类中心的距离，把它划到最小距离的簇；

3.计算新的聚类中心；

4.迭代，直到聚类中心不再更新或达到最大次数。



（3）时间复杂度：$O(k n d t)$。其中k是类别数，n是样本数，d是计算样本之间距离的复杂度，t是迭代次数。



（4）优缺点：

1.优点：原理易懂、实现简单、收敛速度快（这个我不认同）；可解释性强；可调参数少只有k；

2.缺点：聚类效果受k值影响较大；非凸数据集难以收敛；隐含类别不均衡时效果差；迭代算法得到的只是局部最优；对噪声和异常数据敏感。



（5）改进：随机初始化k值影响效果 + 计算样本点到质心的距离耗时（从这两方面优化）

**KMeans++**，KMeans++在选择k个聚类中心时有以下改进：

​	假设已经选择好前n个聚类中心点，在选择第n+1个聚类中心时，距离这前n个聚类中心点越远的点有越大的概率被选中（选择第1个点时还是按照KMeans的方法随机选择）。



KMeans++的k值如何确定（k最适合的值）

KMeans的评估函数是SSE（Sum of Square Error 和方差），即计算所有样本点到其聚类中心的距离的平方和。显然，k越大，SSE越小。但是这样的话，如何选择最适合的k值呢？
$$
SSE = \sum^N_{i=1} \sum^K_{k=1} w_{i,k} \ ||x_i - \mu_k||^2_2 \\
w_{i,k} = \left\{ \begin{array}{c} 
1, & if \ x_i \in k \\
0, & else
\end{array} \right.
$$


1.经验判断。

2.肘部法则：选择SSE减幅最小的k值。比如下图，选择k=3的时候。当 K < 3 时，曲线急速下降；当 K > 3 时，曲线趋于平稳，通过手肘法我们认为拐点 3 为 K 的最佳值。

<img src="https://img-blog.csdn.net/20171128165957366?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VpeGluXzM5ODc1MTgx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="zoom:67%;" />

上图曲线类似于人的手肘，“肘关节”部分对应的 K 值就是最恰当的 K 值，但是并不是所有代价函数曲线都存在明显的“肘关节”。



KMeans也是一种EM算法。EM 算法的缺点就是，容易陷入局部极小值，这也是 KMeans 有时会得到局部最优解的原因。





#### 特征工程 ####

---

标准化：均值为0，方差为1。$std(x) = \frac{x - u}{\sigma}$。数据范围相对于归一化更广。标准化的作用是使数据**服从同一种分布**。

归一化：数据范围压缩到[0, 1]，$norm(x) = \frac{x - min}{max - min}$。归一化的作用是对数据进行**缩放**。



**L1和L2正则化**

|      | L1范数（Lasso回归）                            | L2范数（Ridge回归）                            |
| ---- | ---------------------------------------- | ---------------------------------------- |
| 表示意义 | 各个参数绝对值之和                                | 各个参数的平方和                                 |
| 先验分布 | 服从拉普拉斯分布$\frac{1}{2 \lambda} e^{\frac{-|x - \mu|}{\lambda}}$ | 服从高斯分布$\frac{1}{\sqrt{2 \pi} \sigma} e^{\frac{-(x - \mu)^2}{2 \sigma^2}}$ |
| 功能   | 使参数稀疏化，有特征选择的功能                          | 使参数接近于0，但不全等于0                           |



**过拟合**

（1）产生过拟合的原因：1.数据有噪声；2.数据样本不足，训练样本优先；3.过度训练。

（2）防止过拟合方法：

1.从数据集角度出发：数据增强；采样法；SMOTE或者GAN。

2.从算法角度出发：早停法；dropout；集成学习。

3.从目标函数角度出发：损失函数（比如Focal Loss），正则项。



**特征选择**

什么特征才是好特征：1.特征分布不变；2.特征覆盖率高；3.特征之间相关性小（协方差）。







#### SVM ####

---

1.原理：SVM试图寻找一个超平面使正负样本分开，并使得几何间隔最大。

分清以下三种情况：

（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分SVM；

（2）当训练样本近似线性可分时，引入**松弛变量**，通过软间隔最大化，学习一个线性分类器，即线性SVM；

（3）当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个**非**线性分类器，即**非**线性SVM。



2.硬间隔SVM推导

假设分离超平面为$wx + b = 0$

则有

1）函数间隔$\hat{y} = y(wx + b)$，默认为是二分类问题，y的取值为1和-1，保证了无论样本属于哪一类，$y(wx + b)$都是大于0的。

2）几何间隔$y = \frac{\hat{y}}{||w||}$



目的就是要使$max_{w,b} \frac{\hat{y}}{||w||}$

由于函数间隔取值并不影响最优化问题的解，因此令$\hat{y} = 1$
$$
max_{w,b} \frac{\hat{y}}{||w||} \\
= max_{w,b} \frac{1}{||w||} \\
= min_{w,b} \frac{1}{2} ||w||^2
$$
由于令$\hat{y}=1$，故$s.t. \ \ y_i(wx_i + b) - 1 \geq 0 \ \ i = 1, 2, ...$

定义拉格朗日函数 $L(w,b,\lambda) = \frac{1}{2}||w||^2 + \sum^N_{i=1} \lambda_i - \sum^N_{i=1} \lambda_i y_i(wx_i + b)$



由拉格朗日对偶性 $max_{\lambda} min_{w,b} L(w, b, \lambda)$



开始求解：

①求$min_{w,b} L(w, b, \lambda)$

分别对w和b求导，并令值为0
$$
\grad_w L(w,b,\lambda) = w - \sum^N_{i=1} \lambda_i y_i x_i = 0\\
\grad_b L(w,b,\lambda) = - \sum^N_{i=1} \lambda_i y_i = 0
$$
即求得
$$
w = \sum^N_{i=1} \lambda_i y_i x_i \\
\sum^N_{i=1} \lambda_i y_i = 0
$$
将原式拆开
$$
min_{w,b} L(w, b, \lambda) = \frac{1}{2}||w||^2 + \sum^N_{i=1} \lambda_i - \sum^N_{i=1} \lambda_i y_i wx_i + \sum^N_{i=1} \lambda_i y_i b
$$
然后把$\grad_w和\grad_b$带入式子中，得
$$
min_{w,b} L(w, b, \lambda) = - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j x_i x_j + \sum^N_{i=1} \lambda_i
$$

注意：这里的$x_i, x_j$不一定相同，一个是求w，一个是求b。



②对$min_{w,b}L(w, b, \lambda)$对$\lambda$求极大，即使对偶问题
$$
max_{\lambda} [-\frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j x_i x_j + \sum^N_{i=1} \lambda_i] \\
s.t. \ \sum^N_{i=1} \lambda_i y_i = 0 \\
i = 1,2,...
$$
max和负号都转换一下
$$
min_{\lambda} [\frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j x_i x_j - \sum^N_{i=1} \lambda_i]
$$

---

注意公式里的$x_i, x_j$，在定义核函数的时候，所谓的内积指的就是这个地方。对单个$x_i$作映射为$\phi(x_i)$，然后两个$\phi(x_i) * \phi(x_j)$相乘做内积，就是在这个地方映射成高维。

---



直到这里，原问题和对偶问题具有强对偶关系，满足KKT条件。**对偶问题将原始问题中的约束转为了对偶问题中的等式约束**。

对于中间这个式子$\lambda_i (1 - y_i(wx_i + b)) = 0$，称为松弛互补条件（complementary slackness），但是$\lambda_i \neq 0$，所以只能$1 - y_i(wx_i + b) = 0$，找到一个$x_k$满足这个条件，那么这个向量就是支持向量，用它来求得b。



将前面解得出来的$x_i, x_j, y_i, y_j$带入到w的计算式中得到$w^*$
$$
w^* = \sum^N_{i=1} \lambda_i y_i x_i \\
$$
然后根据$1 - y_i(wx_i + b) = 0$，找到一个$x_k和y_k$。带入到$y_k = w^* x_k + b^*$，解得$b^*$
$$
b^* = y_k - \sum^N_{i=1} \lambda_i y_i x_i^T x_k （由b = y - wx解得）
$$
这样分离超平面为$w^* x + b^* = 0$



其中$w^*$为data的线性组合，$x_k$是支持向量，$\lambda = \left( \matrix{\lambda_1 \\ \lambda_2 \\ ... \\ \lambda_n} \right)$，$\lambda$每个维度针对每个维度上的向量而言，大多数的是0，只有在支持向量处才有值。



**SVM为什么可以用对偶问题求解？对偶问题的解和原始问题的解的关系？这样求解的好处是？**

（1）首先SVM求解问题是一个凸二次规划问题（目标函数是二次函数，约束条件是线性。这种优化问题被称为凸二次优化问题），应用对偶问题求解更高效。

（2）其次在满足KKT条件下，对偶问题和原始问题的解是完全等价的。

（3）好处有：1.对偶问题将原始问题的约束条件转化为了对偶问题中的等式约束；2.方便核函数的导入；3.改变了问题的复杂度，由求特征向量w转换为了求比例系数$\lambda$。原始问题求解复杂度与w的维度有关，而对偶问题只有样本数量有关（支持向量）；4.求解更高效，因为求解$\lambda$系数的过程中，只有支持向量才非0，其他向量都为0。



**为什么要选择最大间隔分类器？**

1.数学角度考虑，几何间隔越大，误分类次数越小。
$$
误分类次数 \leq \frac{所有样本中最长的向量值}{几何间隔}
$$
2.感知机利用误分类最小策略，求得分离超平面，解有无数个；线性可分SVM利用间隔最大求得最优分离超平面，求得唯一解，而此时模型鲁棒性高，泛化能力强。



**样本失衡会对SVM的结果产生影响吗？如何解决SVM样本失衡问题？样本比例失衡时，使用什么指标评价分类器的好坏？**

前提：SVM使用的是Hinge Loss损失函数。这里借用一下Focal Loss的思想。

1.样本失衡对结果产生影响，分类超平面会靠近样本少的类别。因为，假设对所有样本使用相同的惩罚因子，而优化目标是最小惩罚量，样本少的类别惩罚量少。

2.解决SVM样本失衡。第一，对不同类别赋予不同的惩罚因子，训练样本少，惩罚因子越大。但是缺点是偏离原样本的概率分布；第二，采样策略，过采样和欠采样；第三，基于核函数解决问题。



**SVM如何解决多分类问题**

`https://www.cnblogs.com/CheeseZH/p/5265959.html`

1.直接法。直接修改目标函数，将多个分类面的参数合并到一个目标函数上，然后进行一次性进行求解。这种方法看似简单，但其计算复杂度比较高，实现起来比较困难，只适合用于小型问题中。

2.间接法。主要是通过组合多个二分类器来实现多分类器的构造，常见的方法有One Vs One和One Vs Rest。

One Vs One。在任意两个样本之间设计一个SVM，比如k个样本，就有k(k-1)/2个SVM分类器。对未知样本进行分类时，得票最多的样本即为未知样本的类别。libsvm使用这个方法。缺点：当类别很多的时候,model的个数是n*(n-1)/2,代价还是相当大的。

One Vs Rest。训练时依次将某类划为正类，其他类为负类，一共需要k个SVM。训练时具有最大分类函数的值的类别是未知样本的类别。缺点：因为训练集是1:M，这种情况下存在biased，因而不是很实用。可以在抽取数据集的时候，从完整的负集中再抽取三分之一作为训练负集。



**SVM适合处理什么样的数据？**

高维，稀疏，样本少，样本类别数均衡的数据。



**SVM为什么对缺失数据敏感？（数据缺失某些特征）**

1.SVM没有缺失值处理策略；

2.SVM希望样本在特征空间中线性可分，特征空间的好坏影响SVM性能；

3.缺失特征数据影响训练结果。

说白了，SVM的好坏取决于支持向量的选择，缺失值和噪声影响支持向量的选择。



**SVM的损失函数 Hinge Loss**
$$
Loss_{Hinge} = max(0, 1 - yf(x)) \\
f(x) = wx + b
$$
SVM损失函数就是Hinge Loss加上正则项
$$
Loss_{SVM} = \sum^N_{i=1} [1 - y_i f(x_i)] + \lambda ||w||^2
$$


**SMO算法实现SVM（思想、步骤、常见问题）**

1.思想。将大的优化问题分解为多个小的优化问题，求解小的优化问题往往更简单，同时顺序求解小问题得出的结果 与 将它们作为整体 求得的结果一致。

2.步骤。（1）选取一对需要更新的变量$\lambda_i, \lambda_j$；（2）固定除此之外的所有变量，求解对偶问题获得新的$\lambda_i, \lambda_j, b$。

3.常见问题：如何选取$\lambda_i, \lambda_j, b$？（1）选择违反KKT条件最严重的$\lambda_i$，在针对这个$\lambda_i$选择最有可能获得最大修正步长的$\lambda_j$；（2）b一般选择支持向量求解的平均值。



**SVM如何解决非线性问题？你所知道的核函数？**

1.当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

2.常用核函数（重点：Sigmoid，RBF高斯径向基函数）

（1）Linear Kernel。线性可分时，特征数量多时。样本数量多再补充一些特征时，linear kernel可以是RBF kernel的特殊情况。$K(x_i, x_j) = x_i^T x_j$，这里$x_i, x_j$分开标记仅仅区分两个不同的向量，都是多维，并不是某一维。

（2）Polynomial Kernel。Image Processing，参数比RBF多，取值范围是(0, inf)。$K(x_i, x_j) = (\gamma x_i^T x_j + r)^d, d > 1$。

（3）Gaussian radial basis function（RBF）。通用，线性不可分，特征维度少，样本数量正常时，在没有先验知识时用，取值范围是[0, 1]。$K(x_i, x_j) = exp(-\gamma ||x_i - x_j||^2)$。

（4）Sigmoid Kernel。生成神经网络，在某些参数下和RBF很像，可能在某些参数下是无效的。$K(x_i, x_j) = tanh(\alpha x_i^T x_j + c)$。

（5） Gaussian Kernel。通用，在没有先验知识时用。$K(x_i, x_j) = exp(- \frac{||x_i - x_j||^2}{2 \sigma^2})$。



**线性核与RBF核的区别？**

1.训练速度。线性核只有惩罚因子一个参数，训练速度快，RBF还需要调节$\gamma$；

2.训练结果。线性核得到的权重w能反映出特征的重要性，由此进行特征选择，RBF无法解释；

3.训练数据。线性核适合样本特征 >> 样本数量的，RBF核相反。（揭示了如何选择核函数）



**SVM和LR的联系和区别**

1.联系：

都是判别式模型；

都是有监督的分类算法；

如果不考虑核函数，都是线性分类算法。

2.区别：

（1）LR可以输出概率，SVM不行；

（2）损失函数不同，即分类机制不同；

（3）SVM通过引入核函数解决非线性问题；LR则主要靠特征构造，必须组合交叉特征，特征离散化。（原因：LR里每个样本点都要参与核计算，计算复杂度太高，故LR通常不用核函数。）

（4）SVM计算复杂，效果比LR好，适用于小数据集；LR计算快，适用于大数据集，用于在线学习；

（5）SVM分类只与分类超平面附近的点有关，LR与所有点都有关系；

（6）SVM是结构风险最小化，LR则是经验风险最小化。（结构风险最小化就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差。为了达到结构风险最小化的目的，最常用的方法就是添加正则项。）为什么说SVM是结构风险最小化，因为SVM的损失函数就是Hinge Loss加上一个正则项。



**SVM如何防止过拟合？**

SVM中过拟合的原因是因为数据集中的异常点，这些点严重偏离正常位置。我们知道，决定SVM最优分类超平面的恰恰是那些占少数的支持向量，如果支持向量中碰巧存在异常点，如果让SVM去拟合这样的数据，最后的超平面就不是最优的。

解决办法：引入松弛变量（slack variable）$\xi$，将SVM公式的约束条件改为
$$
y_i (w x_i + b) \geq 1 - \xi, \ \ i = 1, 2, ...
$$
引入松弛变量使SVM能够容忍异常点的存在，为什么？公式中看出，引入松弛变量后，所有点到超平面的距离约束不需要大于等于1了，比方说大于等于0.8就行了，那么异常点就可以不是支持向量了，它可以作为一个普通点的存在，我们的支持向量和超平面都不会受到它的影响。

但是对异常点态容忍会导致任意超平面都可以是“最优”超平面，所以SVM的目标函数也需要修改，加上松弛变量的平方和，并求最小值，大概是$\hat{y} = y_i (w x_i + b) + \xi^2$。这样就达到一个平衡：既希望松弛变量存在以解决异常点问题，又不希望松弛变量太大导致分类解决太差。



**KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？**

KKT条件有很多，这里只列举对应到线性SVM分类器上的KKT条件：
$$
\lambda_i \geq 0 \\
y_i(w x_i + b) -1 \geq 0 \\
\lambda_i (1 - y_i(w x_i + b)) \geq 0
$$
对于以上的KKT条件可以看出，对于任意的训练样本，总有$\lambda_i=0$或者$y_i(wx_i + b) = 1$。

（1）如果$\lambda_i=0$，带入最终模型，$f(x) = b$，即所有样本对模型没有贡献；

（2）如果$y_i(wx_i + b) = 1$。所对应的样本刚好位于最大间隔边界上，这对应的是支持向量。这也就引出了一个SVM重要性质：最终的模型仅与支持向量有关。







#### PCA ####

---

期望：$E(X) = \mu, E(Y) = \xi$

方差（自己与本身作协方差）：
$$
Var(X) = E((X - \mu)^2) \\
= E(X^2 - 2 \mu X + \mu^2) \\
= E(X^2) - 2\mu E(X) + E(\mu^2) \\
= E(X^2) - 2 E(X) E(X) + \mu^2 \\
= E(X^2) - 2 E^2(X) + E^2(X) \\
= E(X^2) - E^2(X)
$$
协方差：
$$
Cov(X, Y) = E((X - \mu) * (Y - \xi)) \\
= E(XY - \xi X - \mu Y + \mu \xi) \\
= E(XY) - E(\xi X) - E(\mu Y) + E(\mu \xi) \\
= E(XY) - \mu \xi - \mu \xi + \mu \xi \\
= E(XY) - \mu \xi
$$
如果X与Y是独立统计的，那么$E(XY) = E(X)*E(Y) = \mu \xi$，则协方差为0；但是反过来，二者协方差为0，不一定独立统计。



**注意：**PCA在求协方差矩阵的过程中，计算的X，Y是同一个样本中的不同维度（不同特征）之间的协方差，而不是多个样本之间的协方差。

​		这点特别容易理解，因为PCA的目的是剔除相关性较高的冗余特征，而协方差就是为了计算出哪些特征之间的相关性较高。

![](https://img-blog.csdn.net/20180528220720710?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

比如图中一个样本X的协方差矩阵C，有以下几个特点：

1.协方差矩阵是一个实对称矩阵（图中显然看出，关于主对角线对称）；

2.协方差矩阵的主对角线的元素是每个维度（特征）的方差（cov(x, x), ... cov(z, z)这不就是自己的方差吗，显然得出这个结论）；

3.协方差矩阵计算的是一个样本中不同特征之间的协方差，这点跟上面的注意是同一点。



由实对称矩阵的性质，可以轻松得出矩阵C可以进行相似对角化，且不同特征值之间的特征向量必正交。
$$
D^T C D = \Lambda = \left[ \begin{matrix} 
\lambda_1 \\
& \lambda_2 \\
& & ... \\
& & & \lambda_n \\
\end{matrix} \right]
$$


**PCA简介**

（1）作用：用于特征降维，去除冗余和可分性不强的特征；

（2）目标：降维后的各个特征不相关，即特征之间的协方差为0；

（3）原理：基于训练数据X的协方差矩阵（行为样本，列为特征）的特征向量组成的k阶矩阵U，通过XU得到降维后的k阶矩阵Z；

（4）算法步骤：

1.计算训练样本的协方差矩阵C；

2.计算C的特征值和特征向量；

3.将C的特征值降序排列，特征值对应的特征向量也依次排序；

4.假如要得到X的k阶降维矩阵，选取C的前k个特征${u_1, u_2, ..., u_k}$，组成降维转换矩阵U；

5.Z = XU，Z即降维后的矩阵。

<img src="https://img-blog.csdn.net/20180528230505145?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" style="zoom:67%;" />







#### 性能评价 ####

---

**Q：请解释一下AUC。**

先画一个混淆矩阵

<img src="https://pic3.zhimg.com/80/v2-a253b01cf7f141b9ad11eefdf3cf58d3_720w.jpg?source=1940ef5c" style="zoom:67%;" />

首先计算TPR，TPR其实就是recall，即$TPR = \frac{TP}{TP + FN}$，正样本中预测为正样本的概率；

再计算FPR，$FPR = \frac{FP}{FP + TN}$，负样本中预测为负样本的概率。



按照定义，AUC即ROC曲线下的面积，而ROC曲线的横轴是FPRate，纵轴是TPRate，当二者相等时，即y=x，如下图：

<img src="https://pica.zhimg.com/80/v2-41b0ea9ac4ae69eb2b09ccb69d01e083_720w.jpg?source=1940ef5c" style="zoom:67%;" />

表示的意义是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的。

最理想的情况下，**既**没有真实类别为1而错分为0的样本——TPRate一直为1，**也**没有真实类别为0而错分为1的样本——FP rate一直为0，AUC为1，这便是AUC的极大值。图像趋向于左上角。

<img src="https://pic3.zhimg.com/80/v2-1dbbadf0c8c8d83aa9b1caafd98758a2_720w.jpg?source=1940ef5c" style="zoom:67%;" />



**Q：AUC和准确率一定是正相关的吗？有什么内在关系吗？**

首先得知道AUC是怎么计算的，按照置信度从高到低的顺序排序，计算TPR和FPR，然后画出ROC曲线，最后求出AUC，因此AUC描述的是正负样本的相对顺序，与正负例的阈值边界无关。

再看准确率（Accuracy），准确率是通过正负例阈值进行判断的。阈值越高，预测正样本越少，负样本越多，反之亦然。

举个例子：正正负正| 负负正负 -> 正正正负 | 正负负负

AUC变高了，准确率没变。



#### 非平衡数据 ####

---

**Q： 机器学习中有哪些非均衡数据集的处理方法？**

考点：从3个角度回答：1.从数据样本角度 2.从算法角度 3.从评价指标角度



（1）过采样和欠采样

过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法。

欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。

总体上，过抽样和欠抽样更适合**大数据**分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。



（2）正负样本惩罚权重（损失函数角度）

Focal Loss
$$
FL(p_t) = - \alpha_t (1 - p_t)^{\gamma} log(p_t)
$$
$\alpha_t$调整正负样本对损失函数的贡献，相同$\alpha_t$下少数样本上升或者下降的幅度比多数样本要慢。

$(1 - p_t)^{\gamma}$调整分类的难易程度，比如有0.9的把握和0.2的把握将当前物体分类为正样本，显然0.9的把握相较于0.2容易分类一些，所以容易分类的物体对损失函数的贡献就小一些。



（3）通过组合/集成方法解决样本不均衡（算法角度）

​		组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（投票，加权投票等）产生分类预测结果。

​		例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。

​		这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。

​		如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。



（4）通过特征选择解决样本不均衡

​		上述几种方法都是基于数据行的操作（**行采样，列采样**），通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。

​		一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。



**Q：请简述一下SMOTE采样方法是如何来处理非平衡数据的？**

合成抽样技术，SMOTE过抽样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。

主要是基于KNN。下面的步骤是基于样本的特征空间（有多少个特征就有多少维，针对所有维度计算欧氏距离）：

- 1）对于**少数类**中的每一个样本，以欧氏距离为标准计算它到**少数类**样本集中所有样本的距离，得到K近邻；
- 2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本，从其K近邻中随机选择若干个样本， 假设选择的近邻为$\hat{x}$；
- 3）对于每一个随机选出的近邻，分别与原样本按照如下的公式构建新的样本：$x_{new} = x + rand(0, 1) * (\hat{x} - x)$。这里叠将增量叠加在$x$**各个特征维度**上，产生了一个新样本。



**Q：原始的SMOTE算法存在什么问题？如何改进？**

​		原始的SMOTE算法对所有的少数类样本都是一视同仁的，但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升。Borderline-SMOTE便是将原始SMOTE算法和边界信息算法结合的算法。

![image-20220321203652905](C:\Users\87995\AppData\Roaming\Typora\typora-user-images\image-20220321203652905.png)

即边界SMOTE的思路：

（1）确定哪些点是噪声，哪些点是正样本，哪些点是边界（危险）集；

（2）对边界（危险）集进行SMOTE算法。







#### 参考 ####

---

阿药算法 http://ml-union.cn/ml/tree-2.html

博客 https://blog.csdn.net/alicelmx/article/details/88424020

暑期求职 https://blog.csdn.net/alicelmx/article/details/80521482

暑期求职2 https://blog.csdn.net/alicelmx/article/details/88956411
