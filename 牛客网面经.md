### 牛客网面经 ###

---

牛客网上的真题或者讨论者的面经。



**美团2020校招**



2.深度学习训练中梯度消失的原因有哪些？有哪些解决方法？

(答案可参考如下要点，看回答能否准确指出主要原因，并答出部分解决思路)

梯度消失产生的主要原因有：一是使用了深层网络，二是采用了不合适的激活函数。

（1）目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助。而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0，也就是梯度消失。

（2）计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显，原因如果使用sigmoid作为激活函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。

 

解决方法：

（1）pre-training+fine-tunning

此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

（2） 选择relu等梯度大部分落在常数上的激活函数

relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题。

（3）batch normalization

BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失的问题，或者可以理解为BN将输出从饱和区拉到了非饱和区。

（4） 残差网络的捷径（shortcut）

相比较于之前的网络结构，残差网络中有很多跨层连接结构（shortcut），这样的结构在反向传播时多了反向传播的路径，可以一定程度上解决梯度消失的问题。

（5）LSTM的“门（gate）”结构

LSTM全称是长短期记忆网络（long-short term memory networks），LSTM的结构设计可以改善RNN中的梯度消失的问题。主要原因在于LSTM内部复杂的“门”(gates)，LSTM通过它内部的“门”可以在更新的时候“记住”前几次训练的”残留记忆“。



3.在做项目或者参加比赛的时候，经常会遇到过拟合的问题。结合你的实际经历，讲讲你是怎么理解过拟合以及怎么解决过拟合问题的？

答：过拟合：在机器学习模型训练或者深度学习模型训练的过程中，会出现模型在训练集上表现能力好，但是在测试集上表现欠佳，这种现象就是过拟合，常常主要原因是由于数据集中存在噪音数据或者训练样本维度太少或者训练集数量太少导致的。



主要想考察候选人：

①对为什么会出现过拟合的理解，

②实际经历中用到的模型是怎么调整参数来解决过拟合的，有哪些参数可以调，有没有实践经验。



解决方案：

- 增强训练样本集；
- 减少样本集的维度；
- 如果模型复杂度太高，和训练样本集的数量级不匹配，此时需要降低模型复杂度；
- 正则化，尽可能减少参数；
- 添加Dropout





**联想面经**

https://www.nowcoder.com/discuss/929030?source_id=discuss_experience_nctrack&channel=-1

attention



文本向量化有哪些方法



pytorch创建一个模块（nn.Sequential）

```python
import torch
import torch.nn as nn

class ConvBlock(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(ConvBlock, self).__init__()
        
        self.conv = nn.Sequetial(
        	nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(num_features=out_channel),
            nn.ReLU(inplace=True)
        )
        
    def forward(self, x):
        x = self.conv(x)
        return x
    
class Net(nn.Module):
    def __init__(self, num_classes=20):
        super(Net, self).__init__()
        
        # 输入图片大小为[3, 32, 32]
        self.conv = nn.Sequential(
        	ConvBlock(3, 32),
            ConvBlock(32, 64)
        )
        
        # 输出feature map大小为[64, 32, 32]
        self.maxpool = nn.MaxPool2d(kernel_size=2)
        # 经过maxpool后，feature map的大小为[64, 16, 16]
        self.fc = nn.Linear(in_features=16*16*64, out_features=num_classes)
        
    def forward(self, x):
        x = self.conv(x)
        x = self.maxpool(x)
        
        # 展成一维
        x = x.view(-1, 16*16*64)
        x = self.fc(x)
        
        return x
    
if __name__ == '__main__':
    net = Net()
    # print(net)
    
    input = torch.randn(size=(1, 3, 32, 32))
    output = net(input)
    print(output.shape)
```



pytorch初始化需要注意什么（torch.no_grad()）

*有一说一，这个问题问的有问题。既然楼主这么解释了，那就说说torch.no_grad()相关*





说一下正则化方法（BN/LN/GN简单介绍，你的项目中GN的效果为什么好一些）





BN有一个最大缺陷是什么（依赖batchsize，所以后面提出了syncbn）



Linux命令

