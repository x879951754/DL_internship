### 集成学习 ###

---

集成学习下的两个重要策略：Bagging和Boosting。



Bagging：每个分类器都随机从原样本中做**有放回**的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。

Boosting：通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost，GBDT。





Bagging下包括随机森林（RF）

Boosting下包括Adaboost，GBDT。其中GBDT并不是一种特定的算法，泛指一些梯度提升树的算法，例如Xgboost和LGB（LightGBM）。



**叶节点分裂策略**

xgboost构建树的分裂策略是 level-wise。另外一个经典的gbdt实现LightGBM采用的是 leaf-wise。



**行采样和列采样**

可以这么理解：行对应的是各个样本，列对应的是样本每个维度的特征。



**行采样和列采样为什么能够防止过拟合？**

行采样和列采样是随机森林、Xgboost等集成模型中常用的trick。它的主要作用是加快训练速度和防止过拟合。

行采样**增加了不同样本集合的差异性，从而不同基学习器之间的差异性也增大**，这样集成的模型效果会更好。

列采样相当于在做随机特征筛选，进入模型的特征个数越少，模型越简单。根据机器学习泛化误差理论（方差和偏差），模型越简单，泛化性越好。

而当今神经网络通常一股脑把所有特征都丢进到模型中训练，不做任何筛选，并且现在都是深度的网络（模型越深，模型越复杂），这也就意味着必须有足够的数据集来支持复杂的模型训练。







#### 随机森林 ####

---

**决策树**

ID3、C4.5、CART。



$p_k$是第k类样本所占样本集合D的比例，信息熵：
$$
Ent(D_v) = - \sum^{|\gamma|}_{k=1} p_k log_2 (p_k)
$$
a代表选择的特征或属性。集合D中，第v个分支上的节点个数为$D_v$，这里$\frac{|D_v|}{|D|}$代表分支的权重。信息增益：
$$
Gain(D, a) = Ent(D) - \sum^N_{v=1} \frac{|D_v|}{|D|} Ent(D_v)
$$
增益率：
$$
Gain_{ratio}(D, a) = \frac{Gain(D, a)}{IV(a)} \\
IV(a) = - \sum^V_{v=1} \frac{|D_v|}{|D|} log_2 (\frac{|D_v|}{|D|})
$$
基尼指数：
$$
Gini(p) = \sum^{|\gamma|}_{k=1} p_k (1 - p_k) = 1 - \sum^{|\gamma|}_{k=1} p_k^2
$$
选择属性a作为分裂节点，有
$$
Gini(D, a) = \sum^{V}_{v=1} \frac{|D_v|}{|D|} Gini(D_v, a)
$$

构建决策树时，采用信息增益率最高的决策（那个特征进行叶子的分裂）；

Gini(p)表示p的不确定性，值越大表示不确定性越大。因此采用基尼指数最小的决策（那个特征作为叶子的分裂）。



信息熵增益例子 https://blog.csdn.net/guomutian911/article/details/78599450







#### Adaboost ####

---

一般提升方法的思想：

（1）每一轮改变训练数据的权值或者概率分布；

（2）将弱分类器组合成一个强分类器。



而Adaboost的做法（**Adaboost的特点**）：

（1）提高前一轮被弱分类器错误分类样本的权值，降低被正确分类样本的权值；

（2）Adaboost采用**加权**多数表决的方式。加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器权值。



Adaboost的3个步骤：

（1）初始化训练样本的权重，N个样本，每个样本$\frac{1}{N}$；

（2）训练若分类器。用带权值的训练样本训练分类器，第一轮每个样本权值都为$\frac{1}{N}$，用这些样本训练第一个弱分类器；将正确分类样本的权值降低，错误分类样本的权值调高，用这些权值更新后的样本再去训练下一个分类器。依次迭代。

（3）将各个弱分类器组合成强分类器。加大误差分类率小的分类器的权重，降低误差分类率大的分了器权重。



Adaboost推导：

给定一个二分类训练集
$$
T = \{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \}, \ \ y_i \in \{+1, -1\}
$$


**步骤一：**

​		初始化与训练数据的权值分布。
$$
D_1 = (w_{11}, w_{12}, ..., w_{1N}), \ \ w_{1i} = \frac{1}{N}, \ \ i = 1, 2, ..., N
$$
其中下标第一位表示轮次m，比如$D_1$，表示$m=1$第一轮的训练数据的权值，$w_{11}$，表示第一轮第一个训练数据的权重。



**步骤二：**

​		轮次$m = 1, 2, ..., M$，执行以下操作：

（1）使用当前权值分布为$D_m$的训练数据集，学习得到基分类：
$$
G_m(x): \chi \rightarrow \{-1,+1\}
$$
（2）用上一步得到的分类器$G_m$，计算上一步训练数据的误差分类率$e_m$
$$
e_m = P(G_m(x) \neq y_i) = \frac{\sum^{N}_{i=1} w_{mi} I(G_m(x_i) \neq y_i)}{\sum^{N}_{i=1} w_{mi}} = \sum^{N}_{i=1} w_{mi} I(G_m(x_i) \neq y_i)
$$
其中$I$表示在所有实例中，这里的$w_{mi}$表示第m轮中第i个实例的权值，且每轮$\sum^{N}_{i=1} w_{mi} = 1$。分母永远为1，因此$G_m(x)$在加权训练集上的误差分类率可以看做是被$G_m(x)$误分类样本的权值之和。



（3）考虑每个弱分类器的权重系数$\alpha_m$，该系数表示$G_m$在最终分类器中的重要程度。计算公式如下：
$$
a_m= \frac{1}{2} \log \frac{1 - e_m}{e_m}
$$
这里的$log$是自然对数。由表达式可知，当$e_m < \frac{1}{2}$时，$a_m \geq 0$，并且$a_m$随着$e_m$的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大。反之亦然，这正好验证了集成学习中每个个体分类器的**分类精度必须大于0.5**的前提条件。



（4）更新训练集权重分布，为下一轮做准备
$$
D_{m+1} = (w_{m+1,1}, w_{m+1,2}, ..., w_{m+1, N})
$$
其中$w_{m+1, i}$可以写成
$$
w_{m+1, i} = \frac{w_{m,i}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), \ \ i = 1,2,...,N
$$
也可以写成如下表达式：
$$
w_{m+1,i} = \left\{ \begin{matrix} 
	\frac{w_{mi}}{Z_m} e^{-a_m}, & G_m(x_i) = y_i, \\ 
	\frac{w_{mi}}{Z_m} e^{a_m}, & G_m(x_i) \neq y_i 
\end{matrix} \right.
$$
其中$Z_m$是我们引入的一个规范化因子，它的作用在于使$D_{m+1}$称为一个概率分布。
$$
Z_m = \sum^{N}_{i=1} w_{m,i} \ exp(-\alpha y_i G_m(x_i)), \ \ i = 1,2,...,N
$$
这里我们假设$a_m \geq 0$，那么正确分类样本权值为$e^{-a_m} < 1$，而错误分类样本的权值$e^{a_m} > 1$，因此误分类样本在下一轮学习中起更大的作用。

不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。



（5）重复步骤二中的1至4步骤，得到一系列的基分类器$G_m$和分类器权重参数$a_m$。



**步骤三：**

​		将前几步得到的基分类器根据权重参数**线性组合**
$$
f(x) = \sum^{M}_{m=1} \alpha_m G_m(x)
$$
得到最终分类器：
$$
G(x) = sign(f(x)) = sign(\sum^{M}_{m=1} \alpha_m G_m(x))
$$
这里，所有的$\alpha_m$之和并不为1。$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的置信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。



**Adaboost的优缺点**

优点：将弱分类器组合成强分类器，不容易发生过拟合。

缺点：对异常样本点比较敏感，异常样本点通常在迭代过程中会获得很高的权值，影响最终的学习。







#### GBDT ####

---

​		GBDT是论文《greedy function appoximation: a gradient boosting machine》中提出的一个**梯度下降提升树框架**，并不是一种特定的算法，而是泛指所有的梯度提升树算法，如Xgboost和LightGBM等。



用一个例子来引入GBDT：https://blog.csdn.net/emeitu/article/details/69944228



提升方法采用的是**加法模型**和**前向分步算法**来解决分类和回归问题。

而以决策树作为基函数的提升方法称为提升树（boosting tree）。GBDT（Gradient Boosting Decision Tree）就是提升树算法的一种，它使用的基学习器是CART（Classification and Regression Tree），**且是CART中的回归树**。



它是一种迭代的决策树算法，通过多轮迭代，每轮学习都在上一轮训练的残差（用损失函数的负梯度来替代）的基础上进行训练。回归问题中，每轮迭代产生一棵CART回归树，迭代结束时将得到多棵CART回归树，然后把所有的树加总起来就得到了最终的提升树。



**GBDT中的损失函数**

分类模型：指数损失函数。

回归模型：平方损失函数，绝对值损失函数，Huber Loss。



**Q：为什么回归树可以作为GBDT的迭代学习器？**

主要考点：GBDT中的树都是CART回归树，而回归树的结果可以累加。



GBDT中的树都是CART回归树，不是分类树，因为GBDT的核心在于累加所有树的结果作为最终结果，而只有回归树的结果可以累加，分类树的结果进行累加是没有意义的。尽管GBDT调整后也可以用于分类，但这不代表GBDT中用到的决策树是分类树。

由于GBDT的学习过程是通过多轮迭代，每次都在上一轮训练结果的残差的基础上进行学习，于是要求基学习器要足够简单，具有高偏差、低方差的特点。GBDT的基学习器是CART回归树，由于高偏差和简单的要求，每棵CART回归树的深度不会很深。

训练的过程就是通过降低偏差来不断提高最终的提升树进行分类和回归的精度，使整体趋近于低偏差、低方差。最终的提升树就是将每轮训练得到的CART回归树加总求和得到（也就是加法模型）。



**Q：GBDT是如何用于分类问题的？**

主要考点：借用逻辑回归的输出类别预测概率分布的做法。

GBDT的分类算法和回归算法两者在思想上没有区别，只是分类算法由于样本的输出不是连续的值，而是离散的值，一般用$\{-1, 0, 1, ...\}$这样的整数表示，导致无法用样本的输出直接去计算残差（用损失函数的负梯度来替代）。

于是借鉴逻辑回归的输出类别的预测概率分布的做法，在GBDT分类算法中，采用对数似然损失函数来计算残差，并用对数似然函数的负梯度作为残差的近似值，最后用决策树去拟合残差。

二分类GBDT分类算法的损失函数，指数函数
$$
L(y, f(x)) = log(1 + exp(-yf(x)))
$$



**Q：为什么GBDT将CART回归树树分成m棵二叉树（每棵树只有两个叶子节点），而不是求一棵m+1层的二叉树（最多有2m个叶子节点）？**

CART是一个二叉树，也是回归树，同时也是分类树，CART的构成简单明了。

CART只能将一个父节点分为2个子节点。CART用Gini指数来决定如何分裂。
$$
Gini(D) = \sum^{|\gamma|}_{k=1} \sum_{k != k} p_k p_{k'} = 1 - \sum^{|\gamma|}_{k=1} p_k^2
$$


**GBDT中采用的是m棵二叉树，而不是一棵m+1层的二叉树。**

主要考点：GBDT要求基学习器足够简单、高偏差、低方差的特点。

在决策树剪枝算法中，只要允许一棵树的叶子节点足够多，那么在训练数据集上总是能达到100%的准确率，但是这样模型的复杂度非常高，在测试数据集上表现比较差。（过拟合）

而GBDT把一棵树拆成m棵树，限制每棵树只有2个叶子结点，可以很好地避免过拟合问题，同时模型相对简单，因此每棵CART树的深度不会太深。



**Q：GBDT是如何进行正则化的？**

考点：3种正则化方式。

（1）步长或者叫学习率（learning rate），定义为$v$，对于前面的弱学习器的迭代。
$$
f_{t}(x) = f_{t-1}(x) + vh_{k}(x)
$$
其中，$f_{t-1}(x)$是前面多个弱学习器的迭代，$h_k(x)$是当前学习器。

$v$的取值范围为$0 < v \leq 1$，对于同样的训练集学习效果，较小的$v$意味着我们需要更多的弱学习器的迭代次数。通常使用步长（learning rate）和迭代次数一起来决定算法的拟合效果。



（2）子采样比例（subsample）。

取值为$(0, 1]$。这里的子采样和随机森林不同，随机森林采用放回采样，而GBDT不放回采样。如果比例为1，则全部样本都使用，如果取值小于1，只有部分样本去作决策拟合。

选择小于1的比例可以减少方差，但是会增加偏差（毕竟样本数少了，模型达不到效果），因此取值不能太低，推荐在$[0.5, 0.8]$。



（3）对CART回归树进行剪枝。



**Q：GBDT的优势？**

主要考点：特征组合和发现重要特征。

1.特征组合。原始特征经过GBDT转变为高维稀疏特征（GBDT的输出相当于组合了低维特征，得到高维特征或者非线性映射），然后将这些新特征作为FM（Factorization Machine）或LR（Logistic Regression）的输入再次进行拟合。

2.发现重要特征。由于决策树的生成过程就是不断地选择特征、分割特征。因此GBDT很容易得到特征的重要度排序，且解释性很强。

3.泛化能力强。

泛化误差可以分为两部分：方差（Variance）和偏差（bias）。

为了保证低偏差，采用boosting，每一步都在相对于上一轮更加拟合测试数据。为了保证低方差，尽量采用简单的基模型，比如很浅的CART二叉树。两者结合，就能基于泛化性能相当弱的学习器构建出泛华能力很强的集成模型。



**Q：GBDT中的缩减作用是什么？**

主要考点：shrinkage。

shrinkage的思想认为：每走一小步**逐步**逼近结果的方式比每次一大步**快速**逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。
$$
F_i(x) = F_{i-1}(x) + uf_i(x) \ \ (0 \leq u \leq 1)
$$
Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。

**这句话怎么理解呢？**weight就是上式中的u，u减小了，迭代次数增加，基模型数量理所当然会增加。



**Q：为什么基于残差的GBDT不是一个好的选择？**

基于残差（梯度）的GBDT在解决回归问题上不算是一个好选择，一个比较明显的确点就是对异常值过于敏感。

比如用平方损失函数：
$$
L(y,f(x)) = \frac{1}{2} (y - f(x))^2
$$
对异常点的误差就放大太多了。

如果只使用绝对值误差：
$$
L(y, f(x)) = |y - f(x)|
$$
误差也有，但是不像MSE那样再平方更加放大误差。

Huber Loss（有点类似于Smooth L1 Loss）：
$$
L(y, f(x)) = \left\{ \begin{array}{c}
	\frac{1}{2} (y - f(x))^2, & |y - f(x)| \leq \sigma \\
	\sigma (|y - f(x)| - \frac{\sigma}{2}), & |y - f(x)| > 	\sigma
\end{array}\right.
$$






**Q：GBDT中的梯度怎么计算的？是谁对谁的梯度？**

当前损失函数$L(y, f(x))$对当前树$f(x)$的梯度。







#### Xgboost ####

---

Xgboost是基于GBDT的一种算法或者说**工程实现**。

​		首先要知道GBDT是一种基于boosting的集成思想，的加法模型，在训练时采用前向分步算法进行贪婪学习，每次迭代都学习**一棵CART树**来拟合**前t-1棵树**的预测结果与训练样本真实值（标签）的残差。

Xgboost的基本思想和GBDT相同，但是做了一些优化，如默认的缺失值处理，加入了二阶导数信息、正则项、列抽样，并且可以并行计算等。



**Q：Xgboost和GBDT有什么不同？**

（1）基分类器：Xgboost的基分类器不仅支持CART决策树，还支持线性分类器。此时Xgboost相当于带L1和L2正则化项的逻辑回归和线性回归。

（2）导数信息：Xgboost对损失函数做了**二阶泰勒展开**，GBDT只做了一阶导数信息。而且，GBDT对回归和分类问题只有专门的损失函数（回归：MSE，MAE，Huber Loss；分类：指数损失函数），但Xgboost可以自定义损失函数，只需要损失函数一阶、二阶可导。

（3）正则项：Xgboost的目标函数加了正则项，相当于预剪枝，使得学习出来的模型更加不容易过拟合。

（4）列抽样：Xgboost支持列采样，与随机森林类似，用于防止过拟合。

（5）缺失值处理：对树中的每个非叶子节点，Xgboost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。

（6）并行化：注意不是tree维度的并行，而是特征维度的并行。Xgboost预先将每个特征按特征值排好序，存储为块结构（block），分裂节点时可以采用多线程并行查找每个特征的最佳分割点，极大地提升训练速度。

并行化在下面有个问题也提到了，也可以参考：https://www.zhihu.com/question/280568070。不过基本上都是差不多的答案，也并没有深入。



**Q：RF和GBDT的异同点**

相同点：

- 都是由多棵树组成，最终的结果都是由多棵树一起决定。

不同点：

- 集成学习：RF属于bagging思想，而GBDT是boosting思想
- 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差
- 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
- 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)
- 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合
- 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感
- 泛化能力：RF不易过拟合，而GBDT容易过拟合



**Q：XGBoost为什么使用泰勒二阶展开**

- 精准性：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数
- 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导。



**Q：XGBoost的并行化部分是如何实现的?**

（1）Xgboost的并行，并不是说每棵树可以进行并行，其本质上仍然采用boostsing思想，每棵树训练前需要等前面的树训练完才能开始训练。

（2）Xgboost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。



**Q：XGBoost为什么快？**

（1）分块并行。

（2）候选分为点。

（3）CPU cache命中优化。使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的buffer中。

（4）Block处理优化。Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐。



**Q： XGBoost中的一棵树的停止生长条件**

- 当新引入的一次分裂所带来的增益Gain<0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。
- 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。



**Q：XGBoost算法防止过拟合的方法有哪些？**

（1）目标函数添加正则项。叶子节点个数 + 叶子节点权重的L2正则化。

（2）列抽样。训练的时候只用一部分特征（不考虑剩余的block块即可）。

（3）子采样。每轮计算不使用全部样本，使算法更加保守。

（4）shrinkage。减小学习率（步长），为后面的训练留出更多学习空间。

（5）early stopping。如果经过固定的迭代次数后，并没有在验证集上改善性能，停止训练过程。



**Q：XGBoost如何处理不平衡数据**

对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决：

- 第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10；
- 第二种，如果你在意概率（预测得分的合理性），你不能重新平衡数据集（会破坏数据的真实分布），应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。

源码到底是怎么利用scale_pos_weight来平衡样本的呢，是调节权重还是过采样呢？源码中是增大了少数样本的权重。

除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。







#### LightGBM ####

---

LightGBM属于Xgboost上进一步加强。

（1）Xgboost在构建回归树中最优特征分裂，分裂点查找时需要全量的样本参与增益的计算；而LightGBM，提出了单边梯度采样算法，不使用全量样本进行信息增益的计算，以达到减少样本个数的目的，并且减少计算量。

（2）Xgboost没有针对特征维度作额外的操作，而LightGBM通过互斥特征绑定减少特征的个数。减少了特征的个数，就意味着在回归树构建时不需要遍历更多的特征。

（3）在构建数的过程中，

leaf wise（LightGBM） 和 level wise（Xgboost）策略。

leaf wise策略，每次只选择增益最大的节点进行分裂。

level wise策略，对每一层的所有节点进行分裂，其中某些节点增益并不大。

（4）针对类别型特征的优化问题，支持直接使用类别型特征，避免对类别特征进行编码（onehot等）

（5）直方图加速

（6）其他工程优化





参考 https://zhuanlan.zhihu.com/p/137847395





#### 集成学习总结 ####

---



**Q：请介绍下常见的几种集成学习框架：boosting/bagging/stacking**

Bagging的全名叫做Boostrap aggregating，每个基学习器都会对训练集进行**有放回**抽样得到子训练集，比较著名的采样法为0.632自助法。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging常用的综合方法是**投票法**，票数最多的类别为预测类别。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gqq1jwde3bj31140f4jtu.jpg" style="zoom:50%;" />



Boosting的训练过程为阶梯状，基模型的训练时有序的，每个基模型都会在前一个基模型学习基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为**加权法**。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gqq1ogp5cyj31400q1gmo.jpg" style="zoom: 50%;" />



Stacking是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，其预测值作为训练样本的特征值，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终的预测结果。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gqq1pcjyz0j31220u0abl.jpg" style="zoom: 50%;" />





**Q：为什么集成学习会好于单个学习器？**

（1）弱分类器之间存在一定的差异性，这种差异性可以增加模型的鲁棒性，实现更好的效果。

（2）数据集过大或者过小，需要进行划分或者有放回的采样，这样会生成很多子集。利用子集训练多个分类器，最终合成一个大的分类器。

（3）如果数据的划分边界过于复杂，很难使用线性分类器进行描述。那么多个弱分类器可以进行模型融合。

（4）对于多个异构的特征集，很难进行特征融合。那么可以考虑为每个子集训练一个分类器，然后进行模型融合。



**Q：请简述下模型的方差与偏差的含义？**

（1）偏差（Bias）描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；

（2）方差（Variance）描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gqq28snx2qj30jy0r40tq.jpg" style="zoom: 50%;" />



**Q：集成学习中的基模型一定是弱模型吗？**

通常来说，弱模型指的是偏差高（在训练集上准确率低，打不中靶）方差小（防止过拟合能力强，打得稳）的模型，但并不是所有集成学习框架中的模型都是弱模型。

（1）Bagging和Stacking中的基模型都是强模型（偏差低，方差高）；

（2）Boosting中的基模型都是弱模型（偏差高，方差低）。



**Q：模型的总体期望和总体方差**

设模型期望为$u$，方差为$\sigma^2$，模型权重为r，两两模型间的关系为$\rho$。由于Bagging和Boosting的基模型都是线性组成的，那么

模型总体期望：
$$
E(F) = E(\sum^{m}_{i} r_i f_i) = \sum^{m}_{i} r_i E(f_i)
$$
模型总体方差：
$$
Var(F) = Var(\sum^{m}_{i} r_i f_i) \\
= \sum^{m}_{i} Var(r_i  f_i) + \sum^{m}_{i \neq j} Cov(r_i f_i, r_j f_j) \\
= r_i^2\sum^{m}_{i} Var(f_i) + \sum^{m}_{i \neq j} \rho r_i r_j \sqrt{Var(f_i)} \sqrt{Var(f_j)} \\
= m r^2 \sigma^2 + m(m - 1) \rho r^2 \sigma^2 \\
= m r^2 \sigma^2(1 - \rho) + m^2 r^2 \sigma^2 \rho 
$$
其中协方差计算公式：
$$
Cov(X,Y) = E[(X - E[X]) (Y - E(Y))] \\
= E[XY - XE(Y) - YE(X) + E(X)E(Y)] \\
= E(XY) - E[XE(Y) + YE(X)] + E[E(X)E(Y)] \\
= E(XY) - 2E(X)E(Y) + E(X)E(Y)
$$





#### 特征工程 ####

---

机器学习中，特征选择的工程方法？

https://wenku.baidu.com/view/f6ee64177e21af45b207a82d.html







#### 比较 ####

---













#### 参考 ####

---

阿药算法 http://ml-union.cn/ml/tree-2.html

csdn https://blog.csdn.net/xwd18280820053/article/details/68927422